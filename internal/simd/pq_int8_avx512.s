// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT ·buildDistanceTableInt8Avx512(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ codebook+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ out+40(FP), R9

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x487df262; WORD $0x0118 // vbroadcastss	(%rcx), %zmm0
	LONG $0x487dd262; WORD $0x0818 // vbroadcastss	(%r8), %zmm1
	LONG $0x10fa8348 // cmpq	$0x10, %rdx
	JGE LBB1_1
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	JLE LBB1_5
	QUAD $0xfffffffffffeb848; WORD $0x7fff // movabsq	$0x7ffffffffffffffe, %rax # imm = 0x7FFFFFFFFFFFFFFE
	WORD $0x2148; BYTE $0xd0 // andq	%rdx, %rax
	LONG $0x014e8d48 // leaq	0x1(%rsi), %rcx
	WORD $0x3145; BYTE $0xc0 // xorl	%r8d, %r8d
	JMP LBB1_8
LBB1_12:
	LONG $0x117a81c4; WORD $0x8114 // vmovss	%xmm2, (%r9,%r8,4)
	WORD $0xff49; BYTE $0xc0 // incq	%r8
	WORD $0x0148; BYTE $0xd1 // addq	%rdx, %rcx
	LONG $0x00f88149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r8             # imm = 0x100
	JE LBB1_13
LBB1_8:
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	LONG $0x01fa8348 // cmpq	$0x1, %rdx
	JE LBB1_10
LBB1_9:
	LONG $0x5cbe0f46; WORD $0xff11 // movsbl	-0x1(%rcx,%r10), %r11d
	LONG $0x2a52c1c4; BYTE $0xdb // vcvtsi2ss	%r11d, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0x9724 // vmovss	(%rdi,%r10,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0x9764; BYTE $0x04 // vmovss	0x4(%rdi,%r10,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x1cbe0f46; BYTE $0x11 // movsbl	(%rcx,%r10), %r11d
	LONG $0x2a52c1c4; BYTE $0xd3 // vcvtsi2ss	%r11d, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c28349 // addq	$0x2, %r10
	WORD $0x394c; BYTE $0xd0 // cmpq	%r10, %rax
	JNE LBB1_9
LBB1_10:
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JE LBB1_12
	WORD $0x894d; BYTE $0xc3 // movq	%r8, %r11
	LONG $0xdaaf0f4c // imulq	%rdx, %r11
	WORD $0x0149; BYTE $0xf3 // addq	%rsi, %r11
	LONG $0x1cbe0f47; BYTE $0x13 // movsbl	(%r11,%r10), %r11d
	LONG $0x2a52c1c4; BYTE $0xdb // vcvtsi2ss	%r11d, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0x9724 // vmovss	(%rdi,%r10,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0xb961e2c4; BYTE $0xd3 // vfmadd231ss	%xmm3, %xmm3, %xmm2 # xmm2 = (xmm3 * xmm3) + xmm2
	JMP LBB1_12
LBB1_1:
	LONG $0xf0428d48 // leaq	-0x10(%rdx), %rax
	WORD $0x8948; BYTE $0xc1 // movq	%rax, %rcx
	LONG $0xf0e18348 // andq	$-0x10, %rcx
	LONG $0x10418d4c // leaq	0x10(%rcx), %r8
	WORD $0x8949; BYTE $0xc2 // movq	%rax, %r10
	LONG $0x04eac149 // shrq	$0x4, %r10
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	LONG $0xef5a8d4c // leaq	-0x11(%rdx), %r11
	LONG $0xfee28349 // andq	$-0x2, %r10
	LONG $0x11598d48 // leaq	0x11(%rcx), %rbx
	LONG $0x10768d4c // leaq	0x10(%rsi), %r14
	LONG $0x017e8d4c // leaq	0x1(%rsi), %r15
	WORD $0x3145; BYTE $0xe4 // xorl	%r12d, %r12d
	JMP LBB1_2
LBB1_19:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB1_26:
	LONG $0x117a81c4; WORD $0xa114 // vmovss	%xmm2, (%r9,%r12,4)
	WORD $0xff49; BYTE $0xc4 // incq	%r12
	WORD $0x0149; BYTE $0xd6 // addq	%rdx, %r14
	WORD $0x0149; BYTE $0xd7 // addq	%rdx, %r15
	LONG $0x00fc8149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r12            # imm = 0x100
	JE LBB1_13
LBB1_2:
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0x10f88348 // cmpq	$0x10, %rax
	JCC LBB1_14
	WORD $0x3145; BYTE $0xed // xorl	%r13d, %r13d
	JMP LBB1_16
LBB1_14:
	WORD $0x894c; BYTE $0xd5 // movq	%r10, %rbp
	WORD $0x3145; BYTE $0xed // xorl	%r13d, %r13d
LBB1_15:
	QUAD $0xff2e5c21487d9262 // vpmovsxbd	-0x10(%r14,%r13), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0xaf // vmovups	(%rdi,%r13,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	QUAD $0x01af6410487cb162 // vmovups	0x40(%rdi,%r13,4), %zmm4
	LONG $0x4865f262; WORD $0xdaa8 // vfmadd213ps	%zmm2, %zmm3, %zmm3 # zmm3 = (zmm3 * zmm3) + zmm2
	LONG $0x487d9262; WORD $0x1421; BYTE $0x2e // vpmovsxbd	(%r14,%r13), %zmm2
	LONG $0x487cf162; WORD $0xd25b // vcvtdq2ps	%zmm2, %zmm2
	LONG $0x487df262; WORD $0xd1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm2 # zmm2 = (zmm0 * zmm2) + zmm1
	LONG $0x485cf162; WORD $0xd25c // vsubps	%zmm2, %zmm4, %zmm2
	LONG $0x486df262; WORD $0xd3a8 // vfmadd213ps	%zmm3, %zmm2, %zmm2 # zmm2 = (zmm2 * zmm2) + zmm3
	LONG $0x20c58349 // addq	$0x20, %r13
	LONG $0xfec58348 // addq	$-0x2, %rbp
	JNE LBB1_15
LBB1_16:
	WORD $0x894c; BYTE $0xe5 // movq	%r12, %rbp
	LONG $0xeaaf0f48 // imulq	%rdx, %rbp
	WORD $0x0148; BYTE $0xf5 // addq	%rsi, %rbp
	WORD $0x10a8 // testb	$0x10, %al
	JNE LBB1_18
	QUAD $0x002d5c21487db262 // vpmovsxbd	(%rbp,%r13), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0xaf // vmovups	(%rdi,%r13,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	LONG $0x4865f262; WORD $0xd3b8 // vfmadd231ps	%zmm3, %zmm3, %zmm2 # zmm2 = (zmm3 * zmm3) + zmm2
LBB1_18:
	LONG $0x48fdf362; WORD $0xd31b; BYTE $0x01 // vextractf64x4	$0x1, %zmm2, %ymm3
	LONG $0x486cf162; WORD $0xd358 // vaddps	%zmm3, %zmm2, %zmm2
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xdac6e9c5; BYTE $0x01 // vshufpd	$0x1, %xmm2, %xmm2, %xmm3 # xmm3 = xmm2[1,0]
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xda16fac5 // vmovshdup	%xmm2, %xmm3    # xmm3 = xmm2[1,1,3,3]
	LONG $0xdb58eac5 // vaddss	%xmm3, %xmm2, %xmm3
	WORD $0x3949; BYTE $0xd0 // cmpq	%rdx, %r8
	JGE LBB1_19
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JNE LBB1_22
	WORD $0x894d; BYTE $0xc5 // movq	%r8, %r13
	WORD $0x3949; BYTE $0xcb // cmpq	%rcx, %r11
	JE LBB1_26
	JMP LBB1_24
LBB1_22:
	LONG $0x0d6cbe0f; BYTE $0x10 // movsbl	0x10(%rbp,%rcx), %ebp
	LONG $0xd52ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x6410fac5; WORD $0x408f // vmovss	0x40(%rdi,%rcx,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	WORD $0x8949; BYTE $0xdd // movq	%rbx, %r13
	WORD $0x3949; BYTE $0xcb // cmpq	%rcx, %r11
	JE LBB1_26
LBB1_24:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB1_25:
	LONG $0x6cbe0f43; WORD $0xff2f // movsbl	-0x1(%r15,%r13), %ebp
	LONG $0xdd2ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0xaf24 // vmovss	(%rdi,%r13,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0xaf64; BYTE $0x04 // vmovss	0x4(%rdi,%r13,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x2cbe0f43; BYTE $0x2f // movsbl	(%r15,%r13), %ebp
	LONG $0xd52ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c58349 // addq	$0x2, %r13
	WORD $0x394c; BYTE $0xea // cmpq	%r13, %rdx
	JNE LBB1_25
	JMP LBB1_26
LBB1_5:
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
LBB1_6:
	LONG $0x117cc1c4; WORD $0x8104 // vmovups	%ymm0, (%r9,%rax,4)
	LONG $0x08c08348 // addq	$0x8, %rax
	LONG $0x01003d48; WORD $0x0000 // cmpq	$0x100, %rax            # imm = 0x100
	JNE LBB1_6
LBB1_13:
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·findNearestCentroidInt8Avx512(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ codebook+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ outIndex+40(FP), R9

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x10ec8348 // subq	$0x10, %rsp
	LONG $0x240c894c // movq	%r9, (%rsp)
	LONG $0x487df262; WORD $0x0118 // vbroadcastss	(%rcx), %zmm0
	LONG $0x487dd262; WORD $0x0818 // vbroadcastss	(%r8), %zmm1
	LONG $0x10fa8348 // cmpq	$0x10, %rdx
	JGE LBB2_1
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	JLE LBB2_5
	QUAD $0xfffffffffffeb948; WORD $0x7fff // movabsq	$0x7ffffffffffffffe, %rcx # imm = 0x7FFFFFFFFFFFFFFE
	WORD $0x2148; BYTE $0xd1 // andq	%rdx, %rcx
	LONG $0x01468d4c // leaq	0x1(%rsi), %r8
	WORD $0xb341; BYTE $0x01 // movb	$0x1, %r11b
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB2_7
LBB2_11:
	LONG $0xd32ef8c5 // vucomiss	%xmm3, %xmm2
	LONG $0xc1970f41 // seta	%r9b
	WORD $0x0845; BYTE $0xcb // orb	%r9b, %r11b
	LONG $0x01e38041 // andb	$0x1, %r11b
	LONG $0xc2450f49 // cmovneq	%r10, %rax
	LONG $0x927bc1c4; BYTE $0xcb // kmovd	%r11d, %k1
	LONG $0x096ef162; WORD $0xd310 // vmovss	%xmm3, %xmm2, %xmm2 {%k1}
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	WORD $0x0149; BYTE $0xd0 // addq	%rdx, %r8
	WORD $0x3145; BYTE $0xdb // xorl	%r11d, %r11d
	LONG $0x00fa8149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r10            # imm = 0x100
	JE LBB2_12
LBB2_7:
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d
	LONG $0x01fa8348 // cmpq	$0x1, %rdx
	JE LBB2_9
LBB2_8:
	LONG $0x5cbe0f43; WORD $0xff08 // movsbl	-0x1(%r8,%r9), %ebx
	LONG $0xe32acac5 // vcvtsi2ss	%ebx, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x107aa1c4; WORD $0x8f2c // vmovss	(%rdi,%r9,4), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0x107aa1c4; WORD $0x8f6c; BYTE $0x04 // vmovss	0x4(%rdi,%r9,4), %xmm5
	LONG $0xa959e2c4; BYTE $0xe3 // vfmadd213ss	%xmm3, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm3
	LONG $0x1cbe0f43; BYTE $0x08 // movsbl	(%r8,%r9), %ebx
	LONG $0xdb2acac5 // vcvtsi2ss	%ebx, %xmm6, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0xdb5cd2c5 // vsubss	%xmm3, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0x02c18349 // addq	$0x2, %r9
	WORD $0x394c; BYTE $0xc9 // cmpq	%r9, %rcx
	JNE LBB2_8
LBB2_9:
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JE LBB2_11
	WORD $0x894c; BYTE $0xd3 // movq	%r10, %rbx
	LONG $0xdaaf0f48 // imulq	%rdx, %rbx
	WORD $0x0148; BYTE $0xf3 // addq	%rsi, %rbx
	LONG $0x1cbe0f42; BYTE $0x0b // movsbl	(%rbx,%r9), %ebx
	LONG $0xe32acac5 // vcvtsi2ss	%ebx, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x107aa1c4; WORD $0x8f2c // vmovss	(%rdi,%r9,4), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	JMP LBB2_11
LBB2_1:
	LONG $0xf04a8d48 // leaq	-0x10(%rdx), %rcx
	WORD $0x8949; BYTE $0xc8 // movq	%rcx, %r8
	LONG $0xf0e08349 // andq	$-0x10, %r8
	LONG $0x10508d4d // leaq	0x10(%r8), %r10
	WORD $0x8949; BYTE $0xcb // movq	%rcx, %r11
	LONG $0x04ebc149 // shrq	$0x4, %r11
	WORD $0xff49; BYTE $0xc3 // incq	%r11
	LONG $0xef5a8d48 // leaq	-0x11(%rdx), %rbx
	LONG $0xfee38349 // andq	$-0x2, %r11
	LONG $0x11408d49 // leaq	0x11(%r8), %rax
	LONG $0x24448948; BYTE $0x08 // movq	%rax, 0x8(%rsp)
	LONG $0x107e8d4c // leaq	0x10(%rsi), %r15
	LONG $0x01668d4c // leaq	0x1(%rsi), %r12
	WORD $0xb540; BYTE $0x01 // movb	$0x1, %bpl
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB2_2
LBB2_18:
	LONG $0xdc28f8c5 // vmovaps	%xmm4, %xmm3
LBB2_25:
	LONG $0xd32ef8c5 // vucomiss	%xmm3, %xmm2
	LONG $0xc6970f41 // seta	%r14b
	WORD $0x0844; BYTE $0xf5 // orb	%r14b, %bpl
	LONG $0x01e58040 // andb	$0x1, %bpl
	LONG $0xc1450f49 // cmovneq	%r9, %rax
	LONG $0xcd92fbc5 // kmovd	%ebp, %k1
	LONG $0x096ef162; WORD $0xd310 // vmovss	%xmm3, %xmm2, %xmm2 {%k1}
	WORD $0xff49; BYTE $0xc1 // incq	%r9
	WORD $0x0149; BYTE $0xd7 // addq	%rdx, %r15
	WORD $0x0149; BYTE $0xd4 // addq	%rdx, %r12
	WORD $0xed31 // xorl	%ebp, %ebp
	LONG $0x00f98149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r9             # imm = 0x100
	JE LBB2_12
LBB2_2:
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	LONG $0x10f98348 // cmpq	$0x10, %rcx
	JCC LBB2_13
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	JMP LBB2_15
LBB2_13:
	WORD $0x894d; BYTE $0xdd // movq	%r11, %r13
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
LBB2_14:
	QUAD $0xff376421487d9262 // vpmovsxbd	-0x10(%r15,%r14), %zmm4
	LONG $0x487cf162; WORD $0xe45b // vcvtdq2ps	%zmm4, %zmm4
	LONG $0x487df262; WORD $0xe1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm4 # zmm4 = (zmm0 * zmm4) + zmm1
	LONG $0x487cb162; WORD $0x2c10; BYTE $0xb7 // vmovups	(%rdi,%r14,4), %zmm5
	LONG $0x4854f162; WORD $0xe45c // vsubps	%zmm4, %zmm5, %zmm4
	QUAD $0x01b76c10487cb162 // vmovups	0x40(%rdi,%r14,4), %zmm5
	LONG $0x485df262; WORD $0xe3a8 // vfmadd213ps	%zmm3, %zmm4, %zmm4 # zmm4 = (zmm4 * zmm4) + zmm3
	LONG $0x487d9262; WORD $0x1c21; BYTE $0x37 // vpmovsxbd	(%r15,%r14), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x4854f162; WORD $0xdb5c // vsubps	%zmm3, %zmm5, %zmm3
	LONG $0x4865f262; WORD $0xdca8 // vfmadd213ps	%zmm4, %zmm3, %zmm3 # zmm3 = (zmm3 * zmm3) + zmm4
	LONG $0x20c68349 // addq	$0x20, %r14
	LONG $0xfec58349 // addq	$-0x2, %r13
	JNE LBB2_14
LBB2_15:
	WORD $0x894d; BYTE $0xcd // movq	%r9, %r13
	LONG $0xeaaf0f4c // imulq	%rdx, %r13
	WORD $0x0149; BYTE $0xf5 // addq	%rsi, %r13
	WORD $0xc1f6; BYTE $0x10 // testb	$0x10, %cl
	JNE LBB2_17
	QUAD $0x00356421487d9262 // vpmovsxbd	(%r13,%r14), %zmm4
	LONG $0x487cf162; WORD $0xe45b // vcvtdq2ps	%zmm4, %zmm4
	LONG $0x487df262; WORD $0xe1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm4 # zmm4 = (zmm0 * zmm4) + zmm1
	LONG $0x487cb162; WORD $0x2c10; BYTE $0xb7 // vmovups	(%rdi,%r14,4), %zmm5
	LONG $0x4854f162; WORD $0xe45c // vsubps	%zmm4, %zmm5, %zmm4
	LONG $0x485df262; WORD $0xdcb8 // vfmadd231ps	%zmm4, %zmm4, %zmm3 # zmm3 = (zmm4 * zmm4) + zmm3
LBB2_17:
	LONG $0x48fdf362; WORD $0xdc1b; BYTE $0x01 // vextractf64x4	$0x1, %zmm3, %ymm4
	LONG $0x4864f162; WORD $0xdc58 // vaddps	%zmm4, %zmm3, %zmm3
	LONG $0x197de3c4; WORD $0x01dc // vextractf128	$0x1, %ymm3, %xmm4
	LONG $0xdc58e0c5 // vaddps	%xmm4, %xmm3, %xmm3
	LONG $0xe3c6e1c5; BYTE $0x01 // vshufpd	$0x1, %xmm3, %xmm3, %xmm4 # xmm4 = xmm3[1,0]
	LONG $0xdc58e0c5 // vaddps	%xmm4, %xmm3, %xmm3
	LONG $0xe316fac5 // vmovshdup	%xmm3, %xmm4    # xmm4 = xmm3[1,1,3,3]
	LONG $0xe458e2c5 // vaddss	%xmm4, %xmm3, %xmm4
	WORD $0x3949; BYTE $0xd2 // cmpq	%rdx, %r10
	JGE LBB2_18
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JNE LBB2_21
	WORD $0x894d; BYTE $0xd6 // movq	%r10, %r14
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JE LBB2_25
	JMP LBB2_23
LBB2_21:
	LONG $0x74be0f47; WORD $0x1005 // movsbl	0x10(%r13,%r8), %r14d
	LONG $0x2a4ac1c4; BYTE $0xde // vcvtsi2ss	%r14d, %xmm6, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0x876c; BYTE $0x40 // vmovss	0x40(%rdi,%r8,4), %xmm5
	LONG $0xdb5cd2c5 // vsubss	%xmm3, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0xe328f8c5 // vmovaps	%xmm3, %xmm4
	LONG $0x24748b4c; BYTE $0x08 // movq	0x8(%rsp), %r14
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JE LBB2_25
LBB2_23:
	LONG $0xdc28f8c5 // vmovaps	%xmm4, %xmm3
LBB2_24:
	LONG $0x6cbe0f47; WORD $0xff34 // movsbl	-0x1(%r12,%r14), %r13d
	LONG $0x2a4ac1c4; BYTE $0xe5 // vcvtsi2ss	%r13d, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x107aa1c4; WORD $0xb72c // vmovss	(%rdi,%r14,4), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0x107aa1c4; WORD $0xb76c; BYTE $0x04 // vmovss	0x4(%rdi,%r14,4), %xmm5
	LONG $0xa959e2c4; BYTE $0xe3 // vfmadd213ss	%xmm3, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm3
	LONG $0x2cbe0f47; BYTE $0x34 // movsbl	(%r12,%r14), %r13d
	LONG $0x2a4ac1c4; BYTE $0xdd // vcvtsi2ss	%r13d, %xmm6, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0xdb5cd2c5 // vsubss	%xmm3, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0x02c68349 // addq	$0x2, %r14
	WORD $0x394c; BYTE $0xf2 // cmpq	%r14, %rdx
	JNE LBB2_24
	JMP LBB2_25
LBB2_5:
	WORD $0xc031 // xorl	%eax, %eax
LBB2_12:
	LONG $0x240c8b48 // movq	(%rsp), %rcx
	WORD $0x8948; BYTE $0x01 // movq	%rax, (%rcx)
	LONG $0x10c48348 // addq	$0x10, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·squaredL2Int8DequantizedAvx512(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ code+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ out+40(FP), R9

	LONG $0x487df262; WORD $0x0118 // vbroadcastss	(%rcx), %zmm0
	LONG $0x487dd262; WORD $0x0818 // vbroadcastss	(%r8), %zmm1
	LONG $0x10fa8348 // cmpq	$0x10, %rdx
	JGE LBB0_2
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB0_9
LBB0_2:
	LONG $0xf04a8d48 // leaq	-0x10(%rdx), %rcx
	LONG $0x10f98348 // cmpq	$0x10, %rcx
	JCC LBB0_4
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc0 // xorl	%r8d, %r8d
	JMP LBB0_6
LBB0_4:
	WORD $0x8948; BYTE $0xc8 // movq	%rcx, %rax
	LONG $0x04e8c148 // shrq	$0x4, %rax
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	LONG $0xfee08348 // andq	$-0x2, %rax
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc0 // xorl	%r8d, %r8d
LBB0_5:
	LONG $0x487db262; WORD $0x1c21; BYTE $0x06 // vpmovsxbd	(%rsi,%r8), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0x87 // vmovups	(%rdi,%r8,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	QUAD $0x01876410487cb162 // vmovups	0x40(%rdi,%r8,4), %zmm4
	LONG $0x4865f262; WORD $0xdaa8 // vfmadd213ps	%zmm2, %zmm3, %zmm3 # zmm3 = (zmm3 * zmm3) + zmm2
	QUAD $0x01065421487db262 // vpmovsxbd	0x10(%rsi,%r8), %zmm2
	LONG $0x487cf162; WORD $0xd25b // vcvtdq2ps	%zmm2, %zmm2
	LONG $0x487df262; WORD $0xd1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm2 # zmm2 = (zmm0 * zmm2) + zmm1
	LONG $0x485cf162; WORD $0xd25c // vsubps	%zmm2, %zmm4, %zmm2
	LONG $0x486df262; WORD $0xd3a8 // vfmadd213ps	%zmm3, %zmm2, %zmm2 # zmm2 = (zmm2 * zmm2) + zmm3
	LONG $0x20c08349 // addq	$0x20, %r8
	LONG $0xfec08348 // addq	$-0x2, %rax
	JNE LBB0_5
LBB0_6:
	WORD $0x8948; BYTE $0xc8 // movq	%rcx, %rax
	LONG $0xf0e08348 // andq	$-0x10, %rax
	WORD $0xc1f6; BYTE $0x10 // testb	$0x10, %cl
	JNE LBB0_8
	LONG $0x487db262; WORD $0x1c21; BYTE $0x06 // vpmovsxbd	(%rsi,%r8), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0x87 // vmovups	(%rdi,%r8,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	LONG $0x4865f262; WORD $0xd3b8 // vfmadd231ps	%zmm3, %zmm3, %zmm2 # zmm2 = (zmm3 * zmm3) + zmm2
LBB0_8:
	LONG $0x10c08348 // addq	$0x10, %rax
LBB0_9:
	LONG $0x48fdf362; WORD $0xd31b; BYTE $0x01 // vextractf64x4	$0x1, %zmm2, %ymm3
	LONG $0x486cf162; WORD $0xd358 // vaddps	%zmm3, %zmm2, %zmm2
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xdac6e9c5; BYTE $0x01 // vshufpd	$0x1, %xmm2, %xmm2, %xmm3 # xmm3 = xmm2[1,0]
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xda16fac5 // vmovshdup	%xmm2, %xmm3    # xmm3 = xmm2[1,1,3,3]
	LONG $0xdb58eac5 // vaddss	%xmm3, %xmm2, %xmm3
	WORD $0x3948; BYTE $0xc2 // cmpq	%rax, %rdx
	JLE LBB0_10
	WORD $0xd189 // movl	%edx, %ecx
	WORD $0xc129 // subl	%eax, %ecx
	WORD $0xc1f6; BYTE $0x01 // testb	$0x1, %cl
	JNE LBB0_13
	WORD $0x8948; BYTE $0xc1 // movq	%rax, %rcx
	LONG $0xff428d4c // leaq	-0x1(%rdx), %r8
	WORD $0x394c; BYTE $0xc0 // cmpq	%r8, %rax
	JNE LBB0_15
	JMP LBB0_17
LBB0_10:
	LONG $0x117ac1c4; BYTE $0x19 // vmovss	%xmm3, (%r9)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET
LBB0_13:
	LONG $0x060cbe0f // movsbl	(%rsi,%rax), %ecx
	LONG $0xd12ad2c5 // vcvtsi2ss	%ecx, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x2410fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x01488d48 // leaq	0x1(%rax), %rcx
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	LONG $0xff428d4c // leaq	-0x1(%rdx), %r8
	WORD $0x394c; BYTE $0xc0 // cmpq	%r8, %rax
	JE LBB0_17
LBB0_15:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_16:
	LONG $0x0e04be0f // movsbl	(%rsi,%rcx), %eax
	LONG $0xd82ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x2410fac5; BYTE $0x8f // vmovss	(%rdi,%rcx,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x6410fac5; WORD $0x048f // vmovss	0x4(%rdi,%rcx,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x0e44be0f; BYTE $0x01 // movsbl	0x1(%rsi,%rcx), %eax
	LONG $0xd02ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c18348 // addq	$0x2, %rcx
	WORD $0x3948; BYTE $0xca // cmpq	%rcx, %rdx
	JNE LBB0_16
LBB0_17:
	LONG $0x117ac1c4; BYTE $0x11 // vmovss	%xmm2, (%r9)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

