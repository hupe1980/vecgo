// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT ·int4L2DistanceAvx2(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ code+8(FP), SI
	MOVQ dim+16(FP), DX
	MOVQ minVal+24(FP), CX
	MOVQ diff+32(FP), R8
	MOVQ out+40(FP), R9

	LONG $0x04ec8348 // subq	$0x4, %rsp
	LONG $0x888889b8; BYTE $0x3d // movl	$0x3d888889, %eax       # imm = 0x3D888889
	LONG $0xc06ef9c5 // vmovd	%eax, %xmm0
	LONG $0x187de2c4; BYTE $0xc0 // vbroadcastss	%xmm0, %ymm0
	LONG $0x0f0f0fb8; BYTE $0x0f // movl	$0xf0f0f0f, %eax        # imm = 0xF0F0F0F
	LONG $0xc86ef9c5 // vmovd	%eax, %xmm1
	LONG $0x5879e2c4; BYTE $0xc9 // vpbroadcastd	%xmm1, %xmm1
	LONG $0x20fa8348 // cmpq	$0x20, %rdx
	JGE LBB0_4
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	JMP LBB0_2
LBB0_4:
	LONG $0xe457d8c5 // vxorps	%xmm4, %xmm4, %xmm4
	WORD $0xc031 // xorl	%eax, %eax
	WORD $0x8949; BYTE $0xf3 // movq	%rsi, %r11
	LONG $0xed57d0c5 // vxorps	%xmm5, %xmm5, %xmm5
LBB0_5:
	QUAD $0x000000808b180f41 // prefetcht0	0x80(%r11)
	LONG $0x7e7ac1c4; BYTE $0x13 // vmovq	(%r11), %xmm2
	LONG $0xd271e1c5; BYTE $0x04 // vpsrlw	$0x4, %xmm2, %xmm3
	LONG $0xd260e1c5 // vpunpcklbw	%xmm2, %xmm3, %xmm2 # xmm2 = xmm3[0],xmm2[0],xmm3[1],xmm2[1],xmm3[2],xmm2[2],xmm3[3],xmm2[3],xmm3[4],xmm2[4],xmm3[5],xmm2[5],xmm3[6],xmm2[6],xmm3[7],xmm2[7]
	LONG $0xd1dbe9c5 // vpand	%xmm1, %xmm2, %xmm2
	LONG $0xda70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm2, %xmm3     # xmm3 = xmm2[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xd2 // vpmovzxbd	%xmm2, %ymm2    # ymm2 = xmm2[0],zero,zero,zero,xmm2[1],zero,zero,zero,xmm2[2],zero,zero,zero,xmm2[3],zero,zero,zero,xmm2[4],zero,zero,zero,xmm2[5],zero,zero,zero,xmm2[6],zero,zero,zero,xmm2[7],zero,zero,zero
	LONG $0xd25bfcc5 // vcvtdq2ps	%ymm2, %ymm2
	LONG $0x317de2c4; BYTE $0xdb // vpmovzxbd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0x7e7ac1c4; WORD $0x0873 // vmovq	0x8(%r11), %xmm6
	LONG $0xd671c1c5; BYTE $0x04 // vpsrlw	$0x4, %xmm6, %xmm7
	LONG $0xf660c1c5 // vpunpcklbw	%xmm6, %xmm7, %xmm6 # xmm6 = xmm7[0],xmm6[0],xmm7[1],xmm6[1],xmm7[2],xmm6[2],xmm7[3],xmm6[3],xmm7[4],xmm6[4],xmm7[5],xmm6[5],xmm7[6],xmm6[6],xmm7[7],xmm6[7]
	LONG $0xf1dbc9c5 // vpand	%xmm1, %xmm6, %xmm6
	LONG $0xfe70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm6, %xmm7     # xmm7 = xmm6[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xf6 // vpmovzxbd	%xmm6, %ymm6    # ymm6 = xmm6[0],zero,zero,zero,xmm6[1],zero,zero,zero,xmm6[2],zero,zero,zero,xmm6[3],zero,zero,zero,xmm6[4],zero,zero,zero,xmm6[5],zero,zero,zero,xmm6[6],zero,zero,zero,xmm6[7],zero,zero,zero
	LONG $0xf65bfcc5 // vcvtdq2ps	%ymm6, %ymm6
	LONG $0x317de2c4; BYTE $0xff // vpmovzxbd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	LONG $0xff5bfcc5 // vcvtdq2ps	%ymm7, %ymm7
	LONG $0x107c41c4; WORD $0x8004 // vmovups	(%r8,%rax,4), %ymm8
	LONG $0xd259fcc5 // vmulps	%ymm2, %ymm0, %ymm2
	LONG $0xa83de2c4; WORD $0x8114 // vfmadd213ps	(%rcx,%rax,4), %ymm8, %ymm2 # ymm2 = (ymm8 * ymm2) + mem
	LONG $0x107c41c4; WORD $0x8044; BYTE $0x20 // vmovups	0x20(%r8,%rax,4), %ymm8
	LONG $0xcb597cc5 // vmulps	%ymm3, %ymm0, %ymm9
	LONG $0xa83d62c4; WORD $0x814c; BYTE $0x20 // vfmadd213ps	0x20(%rcx,%rax,4), %ymm8, %ymm9 # ymm9 = (ymm8 * ymm9) + mem
	LONG $0x107cc1c4; WORD $0x805c; BYTE $0x40 // vmovups	0x40(%r8,%rax,4), %ymm3
	LONG $0xf659fcc5 // vmulps	%ymm6, %ymm0, %ymm6
	LONG $0xa865e2c4; WORD $0x8174; BYTE $0x40 // vfmadd213ps	0x40(%rcx,%rax,4), %ymm3, %ymm6 # ymm6 = (ymm3 * ymm6) + mem
	LONG $0x107cc1c4; WORD $0x805c; BYTE $0x60 // vmovups	0x60(%r8,%rax,4), %ymm3
	LONG $0xff59fcc5 // vmulps	%ymm7, %ymm0, %ymm7
	LONG $0xa865e2c4; WORD $0x817c; BYTE $0x60 // vfmadd213ps	0x60(%rcx,%rax,4), %ymm3, %ymm7 # ymm7 = (ymm3 * ymm7) + mem
	QUAD $0x00000100878c180f // prefetcht0	0x100(%rdi,%rax,4)
	LONG $0x1c10fcc5; BYTE $0x87 // vmovups	(%rdi,%rax,4), %ymm3
	LONG $0xda5ce4c5 // vsubps	%ymm2, %ymm3, %ymm3
	LONG $0x5410fcc5; WORD $0x2087 // vmovups	0x20(%rdi,%rax,4), %ymm2
	LONG $0x5c6c41c4; BYTE $0xc1 // vsubps	%ymm9, %ymm2, %ymm8
	LONG $0x5410fcc5; WORD $0x4087 // vmovups	0x40(%rdi,%rax,4), %ymm2
	LONG $0xd65cecc5 // vsubps	%ymm6, %ymm2, %ymm2
	LONG $0x7410fcc5; WORD $0x6087 // vmovups	0x60(%rdi,%rax,4), %ymm6
	LONG $0xf75cccc5 // vsubps	%ymm7, %ymm6, %ymm6
	LONG $0xa865e2c4; BYTE $0xdc // vfmadd213ps	%ymm4, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm4
	LONG $0xb83dc2c4; BYTE $0xd8 // vfmadd231ps	%ymm8, %ymm8, %ymm3 # ymm3 = (ymm8 * ymm8) + ymm3
	LONG $0xa86de2c4; BYTE $0xd5 // vfmadd213ps	%ymm5, %ymm2, %ymm2 # ymm2 = (ymm2 * ymm2) + ymm5
	LONG $0xb84de2c4; BYTE $0xd6 // vfmadd231ps	%ymm6, %ymm6, %ymm2 # ymm2 = (ymm6 * ymm6) + ymm2
	LONG $0x20508d4c // leaq	0x20(%rax), %r10
	LONG $0x40c08348 // addq	$0x40, %rax
	LONG $0x10c38349 // addq	$0x10, %r11
	LONG $0xe328fcc5 // vmovaps	%ymm3, %ymm4
	LONG $0xea28fcc5 // vmovaps	%ymm2, %ymm5
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	JLE LBB0_5
LBB0_2:
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	LONG $0x10c88348 // orq	$0x10, %rax
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	JLE LBB0_6
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	LONG $0xe328fcc5 // vmovaps	%ymm3, %ymm4
	JMP LBB0_8
LBB0_6:
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	WORD $0xd149; BYTE $0xeb // shrq	%r11
	WORD $0x0149; BYTE $0xf3 // addq	%rsi, %r11
LBB0_7:
	LONG $0x7e7ac1c4; BYTE $0x23 // vmovq	(%r11), %xmm4
	LONG $0xd471d1c5; BYTE $0x04 // vpsrlw	$0x4, %xmm4, %xmm5
	LONG $0xe460d1c5 // vpunpcklbw	%xmm4, %xmm5, %xmm4 # xmm4 = xmm5[0],xmm4[0],xmm5[1],xmm4[1],xmm5[2],xmm4[2],xmm5[3],xmm4[3],xmm5[4],xmm4[4],xmm5[5],xmm4[5],xmm5[6],xmm4[6],xmm5[7],xmm4[7]
	LONG $0xe1dbd9c5 // vpand	%xmm1, %xmm4, %xmm4
	LONG $0xec70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm4, %xmm5     # xmm5 = xmm4[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xe4 // vpmovzxbd	%xmm4, %ymm4    # ymm4 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero,xmm4[4],zero,zero,zero,xmm4[5],zero,zero,zero,xmm4[6],zero,zero,zero,xmm4[7],zero,zero,zero
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0x317de2c4; BYTE $0xed // vpmovzxbd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	LONG $0xed5bfcc5 // vcvtdq2ps	%ymm5, %ymm5
	LONG $0x107c81c4; WORD $0x9034 // vmovups	(%r8,%r10,4), %ymm6
	LONG $0xe459fcc5 // vmulps	%ymm4, %ymm0, %ymm4
	LONG $0xa84da2c4; WORD $0x9124 // vfmadd213ps	(%rcx,%r10,4), %ymm6, %ymm4 # ymm4 = (ymm6 * ymm4) + mem
	LONG $0x107c81c4; WORD $0x9074; BYTE $0x20 // vmovups	0x20(%r8,%r10,4), %ymm6
	LONG $0xed59fcc5 // vmulps	%ymm5, %ymm0, %ymm5
	LONG $0xa84da2c4; WORD $0x916c; BYTE $0x20 // vfmadd213ps	0x20(%rcx,%r10,4), %ymm6, %ymm5 # ymm5 = (ymm6 * ymm5) + mem
	LONG $0x107ca1c4; WORD $0x9734 // vmovups	(%rdi,%r10,4), %ymm6
	LONG $0xe45cccc5 // vsubps	%ymm4, %ymm6, %ymm4
	LONG $0x107ca1c4; WORD $0x9774; BYTE $0x20 // vmovups	0x20(%rdi,%r10,4), %ymm6
	LONG $0xed5cccc5 // vsubps	%ymm5, %ymm6, %ymm5
	LONG $0xa85de2c4; BYTE $0xe3 // vfmadd213ps	%ymm3, %ymm4, %ymm4 # ymm4 = (ymm4 * ymm4) + ymm3
	LONG $0xb855e2c4; BYTE $0xe5 // vfmadd231ps	%ymm5, %ymm5, %ymm4 # ymm4 = (ymm5 * ymm5) + ymm4
	LONG $0x10428d49 // leaq	0x10(%r10), %rax
	LONG $0x20c28349 // addq	$0x20, %r10
	LONG $0x08c38349 // addq	$0x8, %r11
	LONG $0xdc28fcc5 // vmovaps	%ymm4, %ymm3
	WORD $0x3949; BYTE $0xd2 // cmpq	%rdx, %r10
	WORD $0x8949; BYTE $0xc2 // movq	%rax, %r10
	JLE LBB0_7
LBB0_8:
	LONG $0xc458ecc5 // vaddps	%ymm4, %ymm2, %ymm0
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$0x1, %ymm0, %xmm1
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xd07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm2
	LONG $0x8889ba41; WORD $0x3d88 // movl	$0x3d888889, %r10d      # imm = 0x3D888889
	LONG $0x6e7941c4; BYTE $0xfa // vmovd	%r10d, %xmm15
	LONG $0x3c117ac5; BYTE $0x24 // vmovss	%xmm15, (%rsp)
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	JGE LBB0_9
	LONG $0x0410fac5; BYTE $0x24 // vmovss	(%rsp), %xmm0
	WORD $0x8949; BYTE $0xc2 // movq	%rax, %r10
	WORD $0xd149; BYTE $0xea // shrq	%r10
	WORD $0x014c; BYTE $0xd6 // addq	%r10, %rsi
	JMP LBB0_11
LBB0_13:
	LONG $0x02c08348 // addq	$0x2, %rax
	WORD $0xff48; BYTE $0xc6 // incq	%rsi
	LONG $0xd128f8c5 // vmovaps	%xmm1, %xmm2
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	JGE LBB0_14
LBB0_11:
	LONG $0x16b60f44 // movzbl	(%rsi), %r10d
	WORD $0x8945; BYTE $0xd3 // movl	%r10d, %r11d
	LONG $0x04ebc041 // shrb	$0x4, %r11b
	LONG $0xdbb60f45 // movzbl	%r11b, %r11d
	LONG $0x2a2ac1c4; BYTE $0xcb // vcvtsi2ss	%r11d, %xmm10, %xmm1
	LONG $0xc959fac5 // vmulss	%xmm1, %xmm0, %xmm1
	LONG $0x107ac1c4; WORD $0x801c // vmovss	(%r8,%rax,4), %xmm3
	LONG $0xa971e2c4; WORD $0x811c // vfmadd213ss	(%rcx,%rax,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	LONG $0x0c10fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm1
	LONG $0xcb5cf2c5 // vsubss	%xmm3, %xmm1, %xmm1
	LONG $0xa971e2c4; BYTE $0xca // vfmadd213ss	%xmm2, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm2
	LONG $0x01588d4c // leaq	0x1(%rax), %r11
	WORD $0x3949; BYTE $0xd3 // cmpq	%rdx, %r11
	JGE LBB0_13
	LONG $0x0fe28041 // andb	$0xf, %r10b
	LONG $0xd2b60f45 // movzbl	%r10b, %r10d
	LONG $0x2a2ac1c4; BYTE $0xd2 // vcvtsi2ss	%r10d, %xmm10, %xmm2
	LONG $0xd259fac5 // vmulss	%xmm2, %xmm0, %xmm2
	LONG $0x107ac1c4; WORD $0x805c; BYTE $0x04 // vmovss	0x4(%r8,%rax,4), %xmm3
	LONG $0xa969e2c4; WORD $0x815c; BYTE $0x04 // vfmadd213ss	0x4(%rcx,%rax,4), %xmm2, %xmm3 # xmm3 = (xmm2 * xmm3) + mem
	LONG $0x5410fac5; WORD $0x0487 // vmovss	0x4(%rdi,%rax,4), %xmm2
	LONG $0xd35ceac5 // vsubss	%xmm3, %xmm2, %xmm2
	LONG $0xb969e2c4; BYTE $0xca // vfmadd231ss	%xmm2, %xmm2, %xmm1 # xmm1 = (xmm2 * xmm2) + xmm1
	JMP LBB0_13
LBB0_9:
	LONG $0xca28f8c5 // vmovaps	%xmm2, %xmm1
LBB0_14:
	LONG $0x117ac1c4; BYTE $0x09 // vmovss	%xmm1, (%r9)
	LONG $0x04c48348 // addq	$0x4, %rsp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·int4L2DistanceBatchAvx2(SB), NOSPLIT, $0-56
	MOVQ query+0(FP), DI
	MOVQ codes+8(FP), SI
	MOVQ dim+16(FP), DX
	MOVQ n+24(FP), CX
	MOVQ minVal+32(FP), R8
	MOVQ diff+40(FP), R9
	PUSHQ out+48(FP)
	PUSHQ $0

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	BYTE $0x50 // pushq	%rax
	LONG $0x888889b8; BYTE $0x3d // movl	$0x3d888889, %eax       # imm = 0x3D888889
	LONG $0xc06ef9c5 // vmovd	%eax, %xmm0
	LONG $0x0f0f0fb8; BYTE $0x0f // movl	$0xf0f0f0f, %eax        # imm = 0xF0F0F0F
	LONG $0xc86ef9c5 // vmovd	%eax, %xmm1
	LONG $0x888889b8; BYTE $0x3d // movl	$0x3d888889, %eax       # imm = 0x3D888889
	LONG $0xf86e79c5 // vmovd	%eax, %xmm15
	LONG $0x7c117ac5; WORD $0x0424 // vmovss	%xmm15, 0x4(%rsp)
	WORD $0x8548; BYTE $0xc9 // testq	%rcx, %rcx
	JLE LBB2_19
	LONG $0x01528d4c // leaq	0x1(%rdx), %r10
	LONG $0x3feac149 // shrq	$0x3f, %r10
	WORD $0x0149; BYTE $0xd2 // addq	%rdx, %r10
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	WORD $0xd149; BYTE $0xfa // sarq	%r10
	LONG $0x187de2c4; BYTE $0xc0 // vbroadcastss	%xmm0, %ymm0
	LONG $0x5879e2c4; BYTE $0xc9 // vpbroadcastd	%xmm1, %xmm1
	LONG $0x5410fac5; WORD $0x0424 // vmovss	0x4(%rsp), %xmm2
	LONG $0x085e8d4c // leaq	0x8(%rsi), %r11
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	WORD $0x8948; BYTE $0xf3 // movq	%rsi, %rbx
	JMP LBB2_2
LBB2_13:
	LONG $0xdc28f8c5 // vmovaps	%xmm4, %xmm3
LBB2_18:
	LONG $0x24448b48; BYTE $0x40 // movq	0x40(%rsp), %rax
	LONG $0x117aa1c4; WORD $0xb01c // vmovss	%xmm3, (%rax,%r14,4)
	WORD $0x014d; BYTE $0xd3 // addq	%r10, %r11
	WORD $0x014c; BYTE $0xd3 // addq	%r10, %rbx
	WORD $0x894d; BYTE $0xfe // movq	%r15, %r14
	WORD $0x3949; BYTE $0xcf // cmpq	%rcx, %r15
	JE LBB2_19
LBB2_2:
	LONG $0x017e8d4d // leaq	0x1(%r14), %r15
	WORD $0x3949; BYTE $0xcf // cmpq	%rcx, %r15
	JGE LBB2_4
	WORD $0x894d; BYTE $0xfc // movq	%r15, %r12
	LONG $0xe2af0f4d // imulq	%r10, %r12
	LONG $0x0c180f42; BYTE $0x26 // prefetcht0	(%rsi,%r12)
LBB2_4:
	LONG $0x20fa8348 // cmpq	$0x20, %rdx
	JGE LBB2_8
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	WORD $0x3145; BYTE $0xed // xorl	%r13d, %r13d
	LONG $0xe457d8c5 // vxorps	%xmm4, %xmm4, %xmm4
	JMP LBB2_6
LBB2_8:
	LONG $0xed57d0c5 // vxorps	%xmm5, %xmm5, %xmm5
	WORD $0x894d; BYTE $0xdc // movq	%r11, %r12
	WORD $0xed31 // xorl	%ebp, %ebp
	LONG $0xf657c8c5 // vxorps	%xmm6, %xmm6, %xmm6
LBB2_9:
	LONG $0x7e7ac1c4; WORD $0x245c; BYTE $0xf8 // vmovq	-0x8(%r12), %xmm3
	LONG $0xd371d9c5; BYTE $0x04 // vpsrlw	$0x4, %xmm3, %xmm4
	LONG $0xdb60d9c5 // vpunpcklbw	%xmm3, %xmm4, %xmm3 # xmm3 = xmm4[0],xmm3[0],xmm4[1],xmm3[1],xmm4[2],xmm3[2],xmm4[3],xmm3[3],xmm4[4],xmm3[4],xmm4[5],xmm3[5],xmm4[6],xmm3[6],xmm4[7],xmm3[7]
	LONG $0xd9dbe1c5 // vpand	%xmm1, %xmm3, %xmm3
	LONG $0x317de2c4; BYTE $0xe3 // vpmovzxbd	%xmm3, %ymm4    # ymm4 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0xdb70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm3, %xmm3     # xmm3 = xmm3[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xdb // vpmovzxbd	%xmm3, %ymm3    # ymm3 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero,xmm3[4],zero,zero,zero,xmm3[5],zero,zero,zero,xmm3[6],zero,zero,zero,xmm3[7],zero,zero,zero
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0x7e7ac1c4; WORD $0x243c // vmovq	(%r12), %xmm7
	LONG $0xd771b9c5; BYTE $0x04 // vpsrlw	$0x4, %xmm7, %xmm8
	LONG $0xff60b9c5 // vpunpcklbw	%xmm7, %xmm8, %xmm7 # xmm7 = xmm8[0],xmm7[0],xmm8[1],xmm7[1],xmm8[2],xmm7[2],xmm8[3],xmm7[3],xmm8[4],xmm7[4],xmm8[5],xmm7[5],xmm8[6],xmm7[6],xmm8[7],xmm7[7]
	LONG $0xf9dbc1c5 // vpand	%xmm1, %xmm7, %xmm7
	LONG $0x317d62c4; BYTE $0xc7 // vpmovzxbd	%xmm7, %ymm8    # ymm8 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	LONG $0x5b7c41c4; BYTE $0xc0 // vcvtdq2ps	%ymm8, %ymm8
	LONG $0xff70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm7, %xmm7     # xmm7 = xmm7[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xff // vpmovzxbd	%xmm7, %ymm7    # ymm7 = xmm7[0],zero,zero,zero,xmm7[1],zero,zero,zero,xmm7[2],zero,zero,zero,xmm7[3],zero,zero,zero,xmm7[4],zero,zero,zero,xmm7[5],zero,zero,zero,xmm7[6],zero,zero,zero,xmm7[7],zero,zero,zero
	LONG $0xff5bfcc5 // vcvtdq2ps	%ymm7, %ymm7
	LONG $0xe459fcc5 // vmulps	%ymm4, %ymm0, %ymm4
	LONG $0x107c41c4; WORD $0xa90c // vmovups	(%r9,%rbp,4), %ymm9
	LONG $0xa85d42c4; WORD $0xa80c // vfmadd213ps	(%r8,%rbp,4), %ymm4, %ymm9 # ymm9 = (ymm4 * ymm9) + mem
	LONG $0x107cc1c4; WORD $0xa964; BYTE $0x20 // vmovups	0x20(%r9,%rbp,4), %ymm4
	LONG $0xdb59fcc5 // vmulps	%ymm3, %ymm0, %ymm3
	LONG $0xa85dc2c4; WORD $0xa85c; BYTE $0x20 // vfmadd213ps	0x20(%r8,%rbp,4), %ymm4, %ymm3 # ymm3 = (ymm4 * ymm3) + mem
	LONG $0x107cc1c4; WORD $0xa964; BYTE $0x40 // vmovups	0x40(%r9,%rbp,4), %ymm4
	LONG $0xc0593cc5 // vmulps	%ymm0, %ymm8, %ymm8
	LONG $0xa85d42c4; WORD $0xa844; BYTE $0x40 // vfmadd213ps	0x40(%r8,%rbp,4), %ymm4, %ymm8 # ymm8 = (ymm4 * ymm8) + mem
	LONG $0x107cc1c4; WORD $0xa964; BYTE $0x60 // vmovups	0x60(%r9,%rbp,4), %ymm4
	LONG $0xff59fcc5 // vmulps	%ymm7, %ymm0, %ymm7
	LONG $0xa85dc2c4; WORD $0xa87c; BYTE $0x60 // vfmadd213ps	0x60(%r8,%rbp,4), %ymm4, %ymm7 # ymm7 = (ymm4 * ymm7) + mem
	LONG $0x2410fcc5; BYTE $0xaf // vmovups	(%rdi,%rbp,4), %ymm4
	LONG $0x5c5cc1c4; BYTE $0xe1 // vsubps	%ymm9, %ymm4, %ymm4
	LONG $0x4c107cc5; WORD $0x20af // vmovups	0x20(%rdi,%rbp,4), %ymm9
	LONG $0xcb5c34c5 // vsubps	%ymm3, %ymm9, %ymm9
	LONG $0x5c10fcc5; WORD $0x40af // vmovups	0x40(%rdi,%rbp,4), %ymm3
	LONG $0x5c64c1c4; BYTE $0xd8 // vsubps	%ymm8, %ymm3, %ymm3
	LONG $0x44107cc5; WORD $0x60af // vmovups	0x60(%rdi,%rbp,4), %ymm8
	LONG $0xff5cbcc5 // vsubps	%ymm7, %ymm8, %ymm7
	LONG $0xa85de2c4; BYTE $0xe5 // vfmadd213ps	%ymm5, %ymm4, %ymm4 # ymm4 = (ymm4 * ymm4) + ymm5
	LONG $0xb835c2c4; BYTE $0xe1 // vfmadd231ps	%ymm9, %ymm9, %ymm4 # ymm4 = (ymm9 * ymm9) + ymm4
	LONG $0xa865e2c4; BYTE $0xde // vfmadd213ps	%ymm6, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm6
	LONG $0xb845e2c4; BYTE $0xdf // vfmadd231ps	%ymm7, %ymm7, %ymm3 # ymm3 = (ymm7 * ymm7) + ymm3
	LONG $0x206d8d4c // leaq	0x20(%rbp), %r13
	LONG $0x10c48349 // addq	$0x10, %r12
	LONG $0x40c58348 // addq	$0x40, %rbp
	LONG $0xec28fcc5 // vmovaps	%ymm4, %ymm5
	LONG $0xf328fcc5 // vmovaps	%ymm3, %ymm6
	WORD $0x3948; BYTE $0xd5 // cmpq	%rdx, %rbp
	WORD $0x894c; BYTE $0xed // movq	%r13, %rbp
	JLE LBB2_9
LBB2_6:
	WORD $0x894d; BYTE $0xec // movq	%r13, %r12
	LONG $0x10cc8349 // orq	$0x10, %r12
	WORD $0x3949; BYTE $0xd4 // cmpq	%rdx, %r12
	JLE LBB2_10
	WORD $0x894d; BYTE $0xec // movq	%r13, %r12
	LONG $0xec28fcc5 // vmovaps	%ymm4, %ymm5
	JMP LBB2_12
LBB2_10:
	WORD $0x894c; BYTE $0xed // movq	%r13, %rbp
	WORD $0xd148; BYTE $0xed // shrq	%rbp
	WORD $0x0148; BYTE $0xdd // addq	%rbx, %rbp
LBB2_11:
	LONG $0x6d7efac5; BYTE $0x00 // vmovq	(%rbp), %xmm5
	LONG $0xd571c9c5; BYTE $0x04 // vpsrlw	$0x4, %xmm5, %xmm6
	LONG $0xed60c9c5 // vpunpcklbw	%xmm5, %xmm6, %xmm5 # xmm5 = xmm6[0],xmm5[0],xmm6[1],xmm5[1],xmm6[2],xmm5[2],xmm6[3],xmm5[3],xmm6[4],xmm5[4],xmm6[5],xmm5[5],xmm6[6],xmm5[6],xmm6[7],xmm5[7]
	LONG $0xe9dbd1c5 // vpand	%xmm1, %xmm5, %xmm5
	LONG $0x317de2c4; BYTE $0xf5 // vpmovzxbd	%xmm5, %ymm6    # ymm6 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	LONG $0xf65bfcc5 // vcvtdq2ps	%ymm6, %ymm6
	LONG $0xed70f9c5; BYTE $0xee // vpshufd	$0xee, %xmm5, %xmm5     # xmm5 = xmm5[2,3,2,3]
	LONG $0x317de2c4; BYTE $0xed // vpmovzxbd	%xmm5, %ymm5    # ymm5 = xmm5[0],zero,zero,zero,xmm5[1],zero,zero,zero,xmm5[2],zero,zero,zero,xmm5[3],zero,zero,zero,xmm5[4],zero,zero,zero,xmm5[5],zero,zero,zero,xmm5[6],zero,zero,zero,xmm5[7],zero,zero,zero
	LONG $0xed5bfcc5 // vcvtdq2ps	%ymm5, %ymm5
	LONG $0xf659fcc5 // vmulps	%ymm6, %ymm0, %ymm6
	LONG $0x107c81c4; WORD $0xa93c // vmovups	(%r9,%r13,4), %ymm7
	LONG $0xa84d82c4; WORD $0xa83c // vfmadd213ps	(%r8,%r13,4), %ymm6, %ymm7 # ymm7 = (ymm6 * ymm7) + mem
	LONG $0x107c81c4; WORD $0xa974; BYTE $0x20 // vmovups	0x20(%r9,%r13,4), %ymm6
	LONG $0xc5597cc5 // vmulps	%ymm5, %ymm0, %ymm8
	LONG $0xa84d02c4; WORD $0xa844; BYTE $0x20 // vfmadd213ps	0x20(%r8,%r13,4), %ymm6, %ymm8 # ymm8 = (ymm6 * ymm8) + mem
	LONG $0x107ca1c4; WORD $0xaf2c // vmovups	(%rdi,%r13,4), %ymm5
	LONG $0xef5cd4c5 // vsubps	%ymm7, %ymm5, %ymm5
	LONG $0x107ca1c4; WORD $0xaf74; BYTE $0x20 // vmovups	0x20(%rdi,%r13,4), %ymm6
	LONG $0x5c4cc1c4; BYTE $0xf0 // vsubps	%ymm8, %ymm6, %ymm6
	LONG $0xa855e2c4; BYTE $0xec // vfmadd213ps	%ymm4, %ymm5, %ymm5 # ymm5 = (ymm5 * ymm5) + ymm4
	LONG $0xb84de2c4; BYTE $0xee // vfmadd231ps	%ymm6, %ymm6, %ymm5 # ymm5 = (ymm6 * ymm6) + ymm5
	LONG $0x10658d4d // leaq	0x10(%r13), %r12
	LONG $0x08c58348 // addq	$0x8, %rbp
	LONG $0x20c58349 // addq	$0x20, %r13
	LONG $0xe528fcc5 // vmovaps	%ymm5, %ymm4
	WORD $0x3949; BYTE $0xd5 // cmpq	%rdx, %r13
	WORD $0x894d; BYTE $0xe5 // movq	%r12, %r13
	JLE LBB2_11
LBB2_12:
	LONG $0xdd58e4c5 // vaddps	%ymm5, %ymm3, %ymm3
	LONG $0x197de3c4; WORD $0x01dc // vextractf128	$0x1, %ymm3, %xmm4
	LONG $0xdc58e0c5 // vaddps	%xmm4, %xmm3, %xmm3
	LONG $0xdb7ce3c5 // vhaddps	%xmm3, %xmm3, %xmm3
	LONG $0xe37ce3c5 // vhaddps	%xmm3, %xmm3, %xmm4
	WORD $0x3949; BYTE $0xd4 // cmpq	%rdx, %r12
	JGE LBB2_13
	WORD $0x894d; BYTE $0xe5 // movq	%r12, %r13
	WORD $0xd149; BYTE $0xed // shrq	%r13
	WORD $0x0149; BYTE $0xdd // addq	%rbx, %r13
	JMP LBB2_15
LBB2_17:
	LONG $0x02c48349 // addq	$0x2, %r12
	WORD $0xff49; BYTE $0xc5 // incq	%r13
	LONG $0xe328f8c5 // vmovaps	%xmm3, %xmm4
	WORD $0x3949; BYTE $0xd4 // cmpq	%rdx, %r12
	JGE LBB2_18
LBB2_15:
	LONG $0x45b60f41; BYTE $0x00 // movzbl	(%r13), %eax
	WORD $0xc589 // movl	%eax, %ebp
	LONG $0x04edc040 // shrb	$0x4, %bpl
	LONG $0xedb60f40 // movzbl	%bpl, %ebp
	LONG $0xdd2aaac5 // vcvtsi2ss	%ebp, %xmm10, %xmm3
	LONG $0xdb59eac5 // vmulss	%xmm3, %xmm2, %xmm3
	LONG $0x107a81c4; WORD $0xa12c // vmovss	(%r9,%r12,4), %xmm5
	LONG $0xa96182c4; WORD $0xa02c // vfmadd213ss	(%r8,%r12,4), %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + mem
	LONG $0x107aa1c4; WORD $0xa71c // vmovss	(%rdi,%r12,4), %xmm3
	LONG $0xdd5ce2c5 // vsubss	%xmm5, %xmm3, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0x246c8d49; BYTE $0x01 // leaq	0x1(%r12), %rbp
	WORD $0x3948; BYTE $0xd5 // cmpq	%rdx, %rbp
	JGE LBB2_17
	WORD $0x0f24 // andb	$0xf, %al
	WORD $0xb60f; BYTE $0xc0 // movzbl	%al, %eax
	LONG $0xe02aaac5 // vcvtsi2ss	%eax, %xmm10, %xmm4
	LONG $0xe459eac5 // vmulss	%xmm4, %xmm2, %xmm4
	LONG $0x107a81c4; WORD $0xa16c; BYTE $0x04 // vmovss	0x4(%r9,%r12,4), %xmm5
	LONG $0xa95982c4; WORD $0xa06c; BYTE $0x04 // vfmadd213ss	0x4(%r8,%r12,4), %xmm4, %xmm5 # xmm5 = (xmm4 * xmm5) + mem
	LONG $0x107aa1c4; WORD $0xa764; BYTE $0x04 // vmovss	0x4(%rdi,%r12,4), %xmm4
	LONG $0xe55cdac5 // vsubss	%xmm5, %xmm4, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	JMP LBB2_17
LBB2_19:
	LONG $0x08c48348 // addq	$0x8, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	POPQ AX
	POPQ AX
	RET

TEXT ·int4L2DistancePrecomputedAvx2(SB), NOSPLIT, $0-40
	MOVQ query+0(FP), DI
	MOVQ code+8(FP), SI
	MOVQ dim+16(FP), DX
	MOVQ lookupTable+24(FP), CX
	MOVQ out+32(FP), R8

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	BYTE $0x50 // pushq	%rax
	LONG $0x2404894c // movq	%r8, (%rsp)
	WORD $0x8949; BYTE $0xf0 // movq	%rsi, %r8
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0x08fa8348 // cmpq	$0x8, %rdx
	JGE LBB1_2
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	JMP LBB1_4
LBB1_2:
	LONG $0x000070b8; BYTE $0x00 // movl	$0x70, %eax
	WORD $0x3145; BYTE $0xdb // xorl	%r11d, %r11d
	WORD $0x894c; BYTE $0xc6 // movq	%r8, %rsi
LBB1_3:
	WORD $0x8b44; BYTE $0x16 // movl	(%rsi), %r10d
	WORD $0x8945; BYTE $0xd1 // movl	%r10d, %r9d
	LONG $0x04e9c141 // shrl	$0x4, %r9d
	WORD $0x8945; BYTE $0xd6 // movl	%r10d, %r14d
	LONG $0x0fe68341 // andl	$0xf, %r14d
	LONG $0x0fe18341 // andl	$0xf, %r9d
	WORD $0x0149; BYTE $0xc1 // addq	%rax, %r9
	WORD $0x0149; BYTE $0xc6 // addq	%rax, %r14
	WORD $0x8945; BYTE $0xd7 // movl	%r10d, %r15d
	LONG $0x08efc141 // shrl	$0x8, %r15d
	WORD $0x8945; BYTE $0xd4 // movl	%r10d, %r12d
	LONG $0x0cecc141 // shrl	$0xc, %r12d
	LONG $0x0fe78341 // andl	$0xf, %r15d
	LONG $0x0fe48341 // andl	$0xf, %r12d
	WORD $0x0149; BYTE $0xc4 // addq	%rax, %r12
	WORD $0x0149; BYTE $0xc7 // addq	%rax, %r15
	WORD $0x8945; BYTE $0xd5 // movl	%r10d, %r13d
	LONG $0x10edc141 // shrl	$0x10, %r13d
	WORD $0x8944; BYTE $0xd5 // movl	%r10d, %ebp
	WORD $0xedc1; BYTE $0x14 // shrl	$0x14, %ebp
	LONG $0x0fe58341 // andl	$0xf, %r13d
	WORD $0xe583; BYTE $0x0f // andl	$0xf, %ebp
	WORD $0x0148; BYTE $0xc5 // addq	%rax, %rbp
	WORD $0x0149; BYTE $0xc5 // addq	%rax, %r13
	WORD $0x8944; BYTE $0xd3 // movl	%r10d, %ebx
	WORD $0xebc1; BYTE $0x18 // shrl	$0x18, %ebx
	LONG $0x1ceac141 // shrl	$0x1c, %r10d
	WORD $0xe383; BYTE $0x0f // andl	$0xf, %ebx
	WORD $0x0149; BYTE $0xc2 // addq	%rax, %r10
	QUAD $0xffff40a98c10fac5; BYTE $0xff // vmovss	-0xc0(%rcx,%rbp,4), %xmm1
	QUAD $0x1080a94c2171a3c4 // vinsertps	$0x10, -0x80(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	QUAD $0x20c0914c2171a3c4 // vinsertps	$0x20, -0x40(%rcx,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	WORD $0x0148; BYTE $0xc3 // addq	%rax, %rbx
	LONG $0x2171e3c4; WORD $0x990c; BYTE $0x30 // vinsertps	$0x30, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	QUAD $0xfe408994107aa1c4; WORD $0xffff // vmovss	-0x1c0(%rcx,%r9,4), %xmm2
	QUAD $0xfe80b1942169a3c4; WORD $0xffff; BYTE $0x10 // vinsertps	$0x10, -0x180(%rcx,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	QUAD $0xfec0a1942169a3c4; WORD $0xffff; BYTE $0x20 // vinsertps	$0x20, -0x140(%rcx,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	QUAD $0xff00b9942169a3c4; WORD $0xffff; BYTE $0x30 // vinsertps	$0x30, -0x100(%rcx,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	LONG $0x186de3c4; WORD $0x01c9 // vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	LONG $0x107ca1c4; WORD $0x9f14 // vmovups	(%rdi,%r11,4), %ymm2
	LONG $0xc95cecc5 // vsubps	%ymm1, %ymm2, %ymm1
	LONG $0xb875e2c4; BYTE $0xc1 // vfmadd231ps	%ymm1, %ymm1, %ymm0 # ymm0 = (ymm1 * ymm1) + ymm0
	LONG $0x08538d4d // leaq	0x8(%r11), %r10
	LONG $0x80e88348 // subq	$-0x80, %rax
	LONG $0x04c68348 // addq	$0x4, %rsi
	LONG $0x10c38349 // addq	$0x10, %r11
	WORD $0x3949; BYTE $0xd3 // cmpq	%rdx, %r11
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	JLE LBB1_3
LBB1_4:
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$0x1, %ymm0, %xmm1
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xc87cfbc5 // vhaddps	%xmm0, %xmm0, %xmm1
	WORD $0x3949; BYTE $0xd2 // cmpq	%rdx, %r10
	JGE LBB1_5
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	WORD $0xd148; BYTE $0xe8 // shrq	%rax
	WORD $0x0149; BYTE $0xc0 // addq	%rax, %r8
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	LONG $0x04e0c148 // shlq	$0x4, %rax
	JMP LBB1_7
LBB1_9:
	LONG $0x02c28349 // addq	$0x2, %r10
	WORD $0xff49; BYTE $0xc0 // incq	%r8
	LONG $0x20c08348 // addq	$0x20, %rax
	LONG $0xc828f8c5 // vmovaps	%xmm0, %xmm1
	WORD $0x3949; BYTE $0xd2 // cmpq	%rdx, %r10
	JGE LBB1_10
LBB1_7:
	LONG $0x08b60f45 // movzbl	(%r8), %r9d
	WORD $0x8944; BYTE $0xce // movl	%r9d, %esi
	WORD $0xeec1; BYTE $0x04 // shrl	$0x4, %esi
	WORD $0x0148; BYTE $0xc6 // addq	%rax, %rsi
	LONG $0x107aa1c4; WORD $0x9704 // vmovss	(%rdi,%r10,4), %xmm0
	LONG $0x045cfac5; BYTE $0xb1 // vsubss	(%rcx,%rsi,4), %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x01728d49 // leaq	0x1(%r10), %rsi
	WORD $0x3948; BYTE $0xd6 // cmpq	%rdx, %rsi
	JGE LBB1_9
	LONG $0x0fe18341 // andl	$0xf, %r9d
	WORD $0x0149; BYTE $0xc1 // addq	%rax, %r9
	LONG $0x107aa1c4; WORD $0x974c; BYTE $0x04 // vmovss	0x4(%rdi,%r10,4), %xmm1
	LONG $0x5c72a1c4; WORD $0x894c; BYTE $0x40 // vsubss	0x40(%rcx,%r9,4), %xmm1, %xmm1
	LONG $0xb971e2c4; BYTE $0xc1 // vfmadd231ss	%xmm1, %xmm1, %xmm0 # xmm0 = (xmm1 * xmm1) + xmm0
	JMP LBB1_9
LBB1_5:
	LONG $0xc128f8c5 // vmovaps	%xmm1, %xmm0
LBB1_10:
	LONG $0x24048b48 // movq	(%rsp), %rax
	LONG $0x0011fac5 // vmovss	%xmm0, (%rax)
	LONG $0x08c48348 // addq	$0x8, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

