// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT Â·squaredL2BoundedAvx2(SB), NOSPLIT, $0-48
	MOVQ vec1+0(FP), DI
	MOVQ vec2+8(FP), SI
	MOVQ n+16(FP), DX
	MOVSD bound+24(FP), X0
	MOVQ result+32(FP), CX
	MOVQ exceeded+40(FP), R8

	LONG $0xc957f0c5 // vxorps	%xmm1, %xmm1, %xmm1
	LONG $0x20fa8348 // cmpq	$0x20, %rdx
	JGE LBB0_2
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB0_8
LBB0_2:
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	LONG $0xe457d8c5 // vxorps	%xmm4, %xmm4, %xmm4
	JMP LBB0_3
LBB0_6:
	LONG $0x20418d49 // leaq	0x20(%r9), %rax
	LONG $0x40c18349 // addq	$0x40, %r9
	WORD $0x3949; BYTE $0xd1 // cmpq	%rdx, %r9
	WORD $0x8949; BYTE $0xc1 // movq	%rax, %r9
	JGT LBB0_7
LBB0_3:
	LONG $0xec28fcc5 // vmovaps	%ymm4, %ymm5
	LONG $0xf328fcc5 // vmovaps	%ymm3, %ymm6
	LONG $0xfa28fcc5 // vmovaps	%ymm2, %ymm7
	LONG $0xc1287cc5 // vmovaps	%ymm1, %ymm8
	LONG $0x107ca1c4; WORD $0x8f0c // vmovups	(%rdi,%r9,4), %ymm1
	LONG $0x107ca1c4; WORD $0x8f54; BYTE $0x20 // vmovups	0x20(%rdi,%r9,4), %ymm2
	LONG $0x107ca1c4; WORD $0x8f5c; BYTE $0x40 // vmovups	0x40(%rdi,%r9,4), %ymm3
	LONG $0x5c74a1c4; WORD $0x8e0c // vsubps	(%rsi,%r9,4), %ymm1, %ymm1
	LONG $0x5c6ca1c4; WORD $0x8e54; BYTE $0x20 // vsubps	0x20(%rsi,%r9,4), %ymm2, %ymm2
	LONG $0x5c64a1c4; WORD $0x8e5c; BYTE $0x40 // vsubps	0x40(%rsi,%r9,4), %ymm3, %ymm3
	LONG $0x107ca1c4; WORD $0x8f64; BYTE $0x60 // vmovups	0x60(%rdi,%r9,4), %ymm4
	LONG $0x5c5ca1c4; WORD $0x8e64; BYTE $0x60 // vsubps	0x60(%rsi,%r9,4), %ymm4, %ymm4
	LONG $0xa875c2c4; BYTE $0xc8 // vfmadd213ps	%ymm8, %ymm1, %ymm1 # ymm1 = (ymm1 * ymm1) + ymm8
	LONG $0xa86de2c4; BYTE $0xd7 // vfmadd213ps	%ymm7, %ymm2, %ymm2 # ymm2 = (ymm2 * ymm2) + ymm7
	LONG $0xa865e2c4; BYTE $0xde // vfmadd213ps	%ymm6, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm6
	LONG $0xa85de2c4; BYTE $0xe5 // vfmadd213ps	%ymm5, %ymm4, %ymm4 # ymm4 = (ymm4 * ymm4) + ymm5
	LONG $0x20c1f641 // testb	$0x20, %r9b
	JE LBB0_6
	LONG $0xea58f4c5 // vaddps	%ymm2, %ymm1, %ymm5
	LONG $0xf458e4c5 // vaddps	%ymm4, %ymm3, %ymm6
	LONG $0xee58d4c5 // vaddps	%ymm6, %ymm5, %ymm5
	LONG $0x197de3c4; WORD $0x01ee // vextractf128	$0x1, %ymm5, %xmm6
	LONG $0xee58d0c5 // vaddps	%xmm6, %xmm5, %xmm5
	LONG $0xed7cd3c5 // vhaddps	%xmm5, %xmm5, %xmm5
	LONG $0xed7cd3c5 // vhaddps	%xmm5, %xmm5, %xmm5
	LONG $0xe82ef8c5 // vucomiss	%xmm0, %xmm5
	JLS LBB0_6
	LONG $0x2911fac5 // vmovss	%xmm5, (%rcx)
	LONG $0x000001b8; BYTE $0x00 // movl	$0x1, %eax
	WORD $0x8941; BYTE $0x00 // movl	%eax, (%r8)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET
LBB0_7:
	LONG $0xc958ecc5 // vaddps	%ymm1, %ymm2, %ymm1
	LONG $0xd358dcc5 // vaddps	%ymm3, %ymm4, %ymm2
	LONG $0xc958ecc5 // vaddps	%ymm1, %ymm2, %ymm1
LBB0_8:
	LONG $0x197de3c4; WORD $0x01ca // vextractf128	$0x1, %ymm1, %xmm2
	LONG $0xca58f0c5 // vaddps	%xmm2, %xmm1, %xmm1
	LONG $0xc97cf3c5 // vhaddps	%xmm1, %xmm1, %xmm1
	LONG $0xc97cf3c5 // vhaddps	%xmm1, %xmm1, %xmm1
	WORD $0x8949; BYTE $0xc1 // movq	%rax, %r9
	WORD $0x2949; BYTE $0xd1 // subq	%rdx, %r9
	JGE LBB0_13
	WORD $0x8949; BYTE $0xd2 // movq	%rdx, %r10
	LONG $0x03e28349 // andq	$0x3, %r10
	JE LBB0_11
LBB0_10:
	LONG $0x1410fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm2
	LONG $0x145ceac5; BYTE $0x86 // vsubss	(%rsi,%rax,4), %xmm2, %xmm2
	LONG $0xb969e2c4; BYTE $0xca // vfmadd231ss	%xmm2, %xmm2, %xmm1 # xmm1 = (xmm2 * xmm2) + xmm1
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	WORD $0xff49; BYTE $0xca // decq	%r10
	JNE LBB0_10
LBB0_11:
	LONG $0xfcf98349 // cmpq	$-0x4, %r9
	JHI LBB0_13
LBB0_12:
	LONG $0x1410fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm2
	LONG $0x5c10fac5; WORD $0x0487 // vmovss	0x4(%rdi,%rax,4), %xmm3
	LONG $0x145ceac5; BYTE $0x86 // vsubss	(%rsi,%rax,4), %xmm2, %xmm2
	LONG $0x5c5ce2c5; WORD $0x0486 // vsubss	0x4(%rsi,%rax,4), %xmm3, %xmm3
	LONG $0xa969e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm1
	LONG $0x4c10fac5; WORD $0x0887 // vmovss	0x8(%rdi,%rax,4), %xmm1
	LONG $0x645cf2c5; WORD $0x0886 // vsubss	0x8(%rsi,%rax,4), %xmm1, %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x4c10fac5; WORD $0x0c87 // vmovss	0xc(%rdi,%rax,4), %xmm1
	LONG $0x4c5cf2c5; WORD $0x0c86 // vsubss	0xc(%rsi,%rax,4), %xmm1, %xmm1
	LONG $0xa959e2c4; BYTE $0xe3 // vfmadd213ss	%xmm3, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm3
	LONG $0xa971e2c4; BYTE $0xcc // vfmadd213ss	%xmm4, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm4
	LONG $0x04c08348 // addq	$0x4, %rax
	WORD $0x3948; BYTE $0xc2 // cmpq	%rax, %rdx
	JNE LBB0_12
LBB0_13:
	LONG $0x0911fac5 // vmovss	%xmm1, (%rcx)
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0xc82ef8c5 // vucomiss	%xmm0, %xmm1
	WORD $0x970f; BYTE $0xc0 // seta	%al
	WORD $0x8941; BYTE $0x00 // movl	%eax, (%r8)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

