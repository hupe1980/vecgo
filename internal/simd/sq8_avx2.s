// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT ·sq8L2BatchAvx2(SB), NOSPLIT, $0-56
	MOVQ query+0(FP), DI
	MOVQ codes+8(FP), SI
	MOVQ scales+16(FP), DX
	MOVQ biases+24(FP), CX
	MOVQ dim+32(FP), R8
	MOVQ n+40(FP), R9

	WORD $0x854d; BYTE $0xc9 // testq	%r9, %r9
	JLE LBB0_35
	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x18ec8348 // subq	$0x18, %rsp
	LONG $0x24448b48; BYTE $0x50 // movq	0x50(%rsp), %rax
	LONG $0x08f88349 // cmpq	$0x8, %r8
	JGE LBB0_2
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	WORD $0x854d; BYTE $0xc0 // testq	%r8, %r8
	JLE LBB0_6
	LONG $0x0f10fac5 // vmovss	(%rdi), %xmm1
	LONG $0x06c68348 // addq	$0x6, %rsi
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	JMP LBB0_9
LBB0_16:
	LONG $0x117aa1c4; WORD $0x9014 // vmovss	%xmm2, (%rax,%r10,4)
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	WORD $0x014c; BYTE $0xc6 // addq	%r8, %rsi
	WORD $0x394d; BYTE $0xd1 // cmpq	%r10, %r9
	JE LBB0_34
LBB0_9:
	LONG $0x107aa1c4; WORD $0x921c // vmovss	(%rdx,%r10,4), %xmm3
	LONG $0x107aa1c4; WORD $0x9124 // vmovss	(%rcx,%r10,4), %xmm4
	LONG $0x5ebe0f44; BYTE $0xfa // movsbl	-0x6(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xd3 // vcvtsi2ss	%r11d, %xmm7, %xmm2
	LONG $0xa961e2c4; BYTE $0xd4 // vfmadd213ss	%xmm4, %xmm3, %xmm2 # xmm2 = (xmm3 * xmm2) + xmm4
	LONG $0xd25cf2c5 // vsubss	%xmm2, %xmm1, %xmm2
	LONG $0xa969e2c4; BYTE $0xd0 // vfmadd213ss	%xmm0, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm0
	LONG $0x01f88349 // cmpq	$0x1, %r8
	JE LBB0_16
	LONG $0x5ebe0f44; BYTE $0xfb // movsbl	-0x5(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa961e2c4; BYTE $0xec // vfmadd213ss	%xmm4, %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + xmm4
	LONG $0x7710fac5; BYTE $0x04 // vmovss	0x4(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xd5 // vfmadd231ss	%xmm5, %xmm5, %xmm2 # xmm2 = (xmm5 * xmm5) + xmm2
	LONG $0x02f88349 // cmpq	$0x2, %r8
	JE LBB0_16
	LONG $0x5ebe0f44; BYTE $0xfc // movsbl	-0x4(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa961e2c4; BYTE $0xec // vfmadd213ss	%xmm4, %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + xmm4
	LONG $0x7710fac5; BYTE $0x08 // vmovss	0x8(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xd5 // vfmadd231ss	%xmm5, %xmm5, %xmm2 # xmm2 = (xmm5 * xmm5) + xmm2
	LONG $0x03f88349 // cmpq	$0x3, %r8
	JE LBB0_16
	LONG $0x5ebe0f44; BYTE $0xfd // movsbl	-0x3(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa961e2c4; BYTE $0xec // vfmadd213ss	%xmm4, %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + xmm4
	LONG $0x7710fac5; BYTE $0x0c // vmovss	0xc(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xd5 // vfmadd231ss	%xmm5, %xmm5, %xmm2 # xmm2 = (xmm5 * xmm5) + xmm2
	LONG $0x04f88349 // cmpq	$0x4, %r8
	JE LBB0_16
	LONG $0x5ebe0f44; BYTE $0xfe // movsbl	-0x2(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa961e2c4; BYTE $0xec // vfmadd213ss	%xmm4, %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + xmm4
	LONG $0x7710fac5; BYTE $0x10 // vmovss	0x10(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xd5 // vfmadd231ss	%xmm5, %xmm5, %xmm2 # xmm2 = (xmm5 * xmm5) + xmm2
	LONG $0x05f88349 // cmpq	$0x5, %r8
	JE LBB0_16
	LONG $0x5ebe0f44; BYTE $0xff // movsbl	-0x1(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa961e2c4; BYTE $0xec // vfmadd213ss	%xmm4, %xmm3, %xmm5 # xmm5 = (xmm3 * xmm5) + xmm4
	LONG $0x7710fac5; BYTE $0x14 // vmovss	0x14(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xd5 // vfmadd231ss	%xmm5, %xmm5, %xmm2 # xmm2 = (xmm5 * xmm5) + xmm2
	LONG $0x06f88349 // cmpq	$0x6, %r8
	JE LBB0_16
	LONG $0x1ebe0f44 // movsbl	(%rsi), %r11d
	LONG $0x2a42c1c4; BYTE $0xeb // vcvtsi2ss	%r11d, %xmm7, %xmm5
	LONG $0xa951e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm5, %xmm3 # xmm3 = (xmm5 * xmm3) + xmm4
	LONG $0x6710fac5; BYTE $0x18 // vmovss	0x18(%rdi), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0xb961e2c4; BYTE $0xd3 // vfmadd231ss	%xmm3, %xmm3, %xmm2 # xmm2 = (xmm3 * xmm3) + xmm2
	JMP LBB0_16
LBB0_2:
	LONG $0xf8508d4d // leaq	-0x8(%r8), %r10
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	LONG $0xf8e38349 // andq	$-0x8, %r11
	LONG $0x085b8d49 // leaq	0x8(%r11), %rbx
	WORD $0x894d; BYTE $0xd6 // movq	%r10, %r14
	LONG $0x03eec149 // shrq	$0x3, %r14
	WORD $0xff49; BYTE $0xc6 // incq	%r14
	LONG $0xf7788d4d // leaq	-0x9(%r8), %r15
	LONG $0x247c894c; BYTE $0x08 // movq	%r15, 0x8(%rsp)
	LONG $0xfee68349 // andq	$-0x2, %r14
	LONG $0x2474894c; BYTE $0x10 // movq	%r14, 0x10(%rsp)
	LONG $0x09738d4d // leaq	0x9(%r11), %r14
	LONG $0x2434894c // movq	%r14, (%rsp)
	LONG $0x086e8d4c // leaq	0x8(%rsi), %r13
	LONG $0x016e8d48 // leaq	0x1(%rsi), %rbp
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	JMP LBB0_3
LBB0_27:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_33:
	LONG $0x117aa1c4; WORD $0xb014 // vmovss	%xmm2, (%rax,%r14,4)
	WORD $0xff49; BYTE $0xc6 // incq	%r14
	WORD $0x014d; BYTE $0xc5 // addq	%r8, %r13
	WORD $0x014c; BYTE $0xc5 // addq	%r8, %rbp
	WORD $0x394d; BYTE $0xce // cmpq	%r9, %r14
	JE LBB0_34
LBB0_3:
	LONG $0x187da2c4; WORD $0xb204 // vbroadcastss	(%rdx,%r14,4), %ymm0
	LONG $0x187da2c4; WORD $0xb10c // vbroadcastss	(%rcx,%r14,4), %ymm1
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0x08fa8349 // cmpq	$0x8, %r10
	JCC LBB0_22
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
	JMP LBB0_24
LBB0_22:
	LONG $0x24648b4c; BYTE $0x10 // movq	0x10(%rsp), %r12
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
LBB0_23:
	LONG $0x217d82c4; WORD $0x3d5c; BYTE $0xf8 // vpmovsxbd	-0x8(%r13,%r15), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0xbf24 // vmovups	(%rdi,%r15,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0x107ca1c4; WORD $0xbf64; BYTE $0x20 // vmovups	0x20(%rdi,%r15,4), %ymm4
	LONG $0xa865e2c4; BYTE $0xda // vfmadd213ps	%ymm2, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm2
	LONG $0x217d82c4; WORD $0x3d54; BYTE $0x00 // vpmovsxbd	(%r13,%r15), %ymm2
	LONG $0xd25bfcc5 // vcvtdq2ps	%ymm2, %ymm2
	LONG $0xa87de2c4; BYTE $0xd1 // vfmadd213ps	%ymm1, %ymm0, %ymm2 # ymm2 = (ymm0 * ymm2) + ymm1
	LONG $0xd25cdcc5 // vsubps	%ymm2, %ymm4, %ymm2
	LONG $0xa86de2c4; BYTE $0xd3 // vfmadd213ps	%ymm3, %ymm2, %ymm2 # ymm2 = (ymm2 * ymm2) + ymm3
	LONG $0x10c78349 // addq	$0x10, %r15
	LONG $0xfec48349 // addq	$-0x2, %r12
	JNE LBB0_23
LBB0_24:
	WORD $0x894d; BYTE $0xf4 // movq	%r14, %r12
	LONG $0xe0af0f4d // imulq	%r8, %r12
	WORD $0x0149; BYTE $0xf4 // addq	%rsi, %r12
	LONG $0x08c2f641 // testb	$0x8, %r10b
	JNE LBB0_26
	LONG $0x217d82c4; WORD $0x3c1c // vpmovsxbd	(%r12,%r15), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0xbf24 // vmovups	(%rdi,%r15,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0xb865e2c4; BYTE $0xd3 // vfmadd231ps	%ymm3, %ymm3, %ymm2 # ymm2 = (ymm3 * ymm3) + ymm2
LBB0_26:
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xda7cebc5 // vhaddps	%xmm2, %xmm2, %xmm3
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JGE LBB0_27
	WORD $0x8949; BYTE $0xdf // movq	%rbx, %r15
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JE LBB0_30
	LONG $0x7cbe0f47; WORD $0x081c // movsbl	0x8(%r12,%r11), %r15d
	LONG $0x2a52c1c4; BYTE $0xd7 // vcvtsi2ss	%r15d, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x107aa1c4; WORD $0x9f64; BYTE $0x20 // vmovss	0x20(%rdi,%r11,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	LONG $0x243c8b4c // movq	(%rsp), %r15
LBB0_30:
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB0_33
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_32:
	LONG $0x64be0f46; WORD $0xff3d // movsbl	-0x1(%rbp,%r15), %r12d
	LONG $0x2a52c1c4; BYTE $0xdc // vcvtsi2ss	%r12d, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0xbf24 // vmovss	(%rdi,%r15,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0xbf64; BYTE $0x04 // vmovss	0x4(%rdi,%r15,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x64be0f46; WORD $0x003d // movsbl	(%rbp,%r15), %r12d
	LONG $0x2a52c1c4; BYTE $0xd4 // vcvtsi2ss	%r12d, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c78349 // addq	$0x2, %r15
	WORD $0x394d; BYTE $0xf8 // cmpq	%r15, %r8
	JNE LBB0_32
	JMP LBB0_33
LBB0_6:
	WORD $0x8944; BYTE $0xc9 // movl	%r9d, %ecx
	WORD $0xe183; BYTE $0x07 // andl	$0x7, %ecx
	LONG $0x08f98349 // cmpq	$0x8, %r9
	JCC LBB0_17
	WORD $0xd231 // xorl	%edx, %edx
	JMP LBB0_19
LBB0_17:
	QUAD $0xfffffffffff8ba48; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %rdx # imm = 0x7FFFFFFFFFFFFFF8
	WORD $0x2149; BYTE $0xd1 // andq	%rdx, %r9
	WORD $0xd231 // xorl	%edx, %edx
LBB0_18:
	LONG $0x0411fac5; BYTE $0x90 // vmovss	%xmm0, (%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x0490 // vmovss	%xmm0, 0x4(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x0890 // vmovss	%xmm0, 0x8(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x0c90 // vmovss	%xmm0, 0xc(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x1090 // vmovss	%xmm0, 0x10(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x1490 // vmovss	%xmm0, 0x14(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x1890 // vmovss	%xmm0, 0x18(%rax,%rdx,4)
	LONG $0x4411fac5; WORD $0x1c90 // vmovss	%xmm0, 0x1c(%rax,%rdx,4)
	LONG $0x08c28348 // addq	$0x8, %rdx
	WORD $0x3949; BYTE $0xd1 // cmpq	%rdx, %r9
	JNE LBB0_18
LBB0_19:
	WORD $0x8548; BYTE $0xc9 // testq	%rcx, %rcx
	JE LBB0_34
	LONG $0x90048d48 // leaq	(%rax,%rdx,4), %rax
	WORD $0xd231 // xorl	%edx, %edx
LBB0_21:
	LONG $0x0411fac5; BYTE $0x90 // vmovss	%xmm0, (%rax,%rdx,4)
	WORD $0xff48; BYTE $0xc2 // incq	%rdx
	WORD $0x3948; BYTE $0xd1 // cmpq	%rdx, %rcx
	JNE LBB0_21
LBB0_34:
	LONG $0x18c48348 // addq	$0x18, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
LBB0_35:
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·sq8uL2BatchPerDimensionAvx2(SB), NOSPLIT, $0-56
	MOVQ query+0(FP), DI
	MOVQ codes+8(FP), SI
	MOVQ mins+16(FP), DX
	MOVQ invScales+24(FP), CX
	MOVQ dim+32(FP), R8
	MOVQ n+40(FP), R9

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x18ec8348 // subq	$0x18, %rsp
	LONG $0x240c894c // movq	%r9, (%rsp)
	WORD $0x854d; BYTE $0xc9 // testq	%r9, %r9
	JLE LBB1_40
	LONG $0x244c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r9
	LONG $0xf8508d4d // leaq	-0x8(%r8), %r10
	LONG $0x20f88349 // cmpq	$0x20, %r8
	JGE LBB1_2
	LONG $0x08f88349 // cmpq	$0x8, %r8
	JGE LBB1_10
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	WORD $0x854d; BYTE $0xc0 // testq	%r8, %r8
	JLE LBB1_14
	LONG $0x0a10fac5 // vmovss	(%rdx), %xmm1
	LONG $0x1110fac5 // vmovss	(%rcx), %xmm2
	LONG $0x1f10fac5 // vmovss	(%rdi), %xmm3
	LONG $0x06c68348 // addq	$0x6, %rsi
	WORD $0x3145; BYTE $0xd2 // xorl	%r10d, %r10d
	JMP LBB1_17
LBB1_18:
	LONG $0x244c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r9
LBB1_25:
	LONG $0x117a81c4; WORD $0x9124 // vmovss	%xmm4, (%r9,%r10,4)
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	WORD $0x014c; BYTE $0xc6 // addq	%r8, %rsi
	LONG $0x2414394c // cmpq	%r10, (%rsp)
	JE LBB1_40
LBB1_17:
	LONG $0xfa46b60f // movzbl	-0x6(%rsi), %eax
	LONG $0xe02ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm4
	LONG $0xa969e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm2, %xmm4 # xmm4 = (xmm2 * xmm4) + xmm1
	LONG $0xe45ce2c5 // vsubss	%xmm4, %xmm3, %xmm4
	LONG $0xa959e2c4; BYTE $0xe0 // vfmadd213ss	%xmm0, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm0
	LONG $0x01f88349 // cmpq	$0x1, %r8
	JE LBB1_18
	LONG $0xfb46b60f // movzbl	-0x5(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x04 // vmovss	0x4(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x0472 // vfmadd213ss	0x4(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x04 // vmovss	0x4(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x02f88349 // cmpq	$0x2, %r8
	LONG $0x244c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r9
	JE LBB1_25
	LONG $0xfc46b60f // movzbl	-0x4(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x08 // vmovss	0x8(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x0872 // vfmadd213ss	0x8(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x08 // vmovss	0x8(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x03f88349 // cmpq	$0x3, %r8
	JE LBB1_25
	LONG $0xfd46b60f // movzbl	-0x3(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x0c // vmovss	0xc(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x0c72 // vfmadd213ss	0xc(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x0c // vmovss	0xc(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x04f88349 // cmpq	$0x4, %r8
	JE LBB1_25
	LONG $0xfe46b60f // movzbl	-0x2(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x10 // vmovss	0x10(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x1072 // vfmadd213ss	0x10(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x10 // vmovss	0x10(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x05f88349 // cmpq	$0x5, %r8
	JE LBB1_25
	LONG $0xff46b60f // movzbl	-0x1(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x14 // vmovss	0x14(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x1472 // vfmadd213ss	0x14(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x14 // vmovss	0x14(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x06f88349 // cmpq	$0x6, %r8
	JE LBB1_25
	WORD $0xb60f; BYTE $0x06 // movzbl	(%rsi), %eax
	LONG $0xe82ac2c5 // vcvtsi2ss	%eax, %xmm7, %xmm5
	LONG $0x7110fac5; BYTE $0x18 // vmovss	0x18(%rcx), %xmm6
	LONG $0xa951e2c4; WORD $0x1872 // vfmadd213ss	0x18(%rdx), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	LONG $0x6f10fac5; BYTE $0x18 // vmovss	0x18(%rdi), %xmm5
	LONG $0xee5cd2c5 // vsubss	%xmm6, %xmm5, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	JMP LBB1_25
LBB1_2:
	LONG $0xe0588d4d // leaq	-0x20(%r8), %r11
	WORD $0x894d; BYTE $0xd9 // movq	%r11, %r9
	LONG $0xe0e18349 // andq	$-0x20, %r9
	LONG $0x20c18349 // addq	$0x20, %r9
	LONG $0xff408d49 // leaq	-0x1(%r8), %rax
	LONG $0x24448948; BYTE $0x08 // movq	%rax, 0x8(%rsp)
	LONG $0x187e8d4c // leaq	0x18(%rsi), %r15
	WORD $0x3145; BYTE $0xe4 // xorl	%r12d, %r12d
	WORD $0x8949; BYTE $0xf5 // movq	%rsi, %r13
	JMP LBB1_3
LBB1_8:
	LONG $0xc128f8c5 // vmovaps	%xmm1, %xmm0
LBB1_46:
	LONG $0x24448b48; BYTE $0x50 // movq	0x50(%rsp), %rax
	LONG $0x117aa1c4; WORD $0xa004 // vmovss	%xmm0, (%rax,%r12,4)
	WORD $0xff49; BYTE $0xc4 // incq	%r12
	WORD $0x014d; BYTE $0xc7 // addq	%r8, %r15
	WORD $0x014d; BYTE $0xc5 // addq	%r8, %r13
	LONG $0x24243b4c // cmpq	(%rsp), %r12
	JE LBB1_40
LBB1_3:
	WORD $0x894c; BYTE $0xe5 // movq	%r12, %rbp
	LONG $0xe8af0f49 // imulq	%r8, %rbp
	WORD $0x0148; BYTE $0xf5 // addq	%rsi, %rbp
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0xc957f0c5 // vxorps	%xmm1, %xmm1, %xmm1
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	WORD $0xdb31 // xorl	%ebx, %ebx
LBB1_4:
	LONG $0x317dc2c4; WORD $0x1f64; BYTE $0xe8 // vpmovzxbd	-0x18(%r15,%rbx), %ymm4
	LONG $0x317dc2c4; WORD $0x1f6c; BYTE $0xf0 // vpmovzxbd	-0x10(%r15,%rbx), %ymm5
	LONG $0x317dc2c4; WORD $0x1f74; BYTE $0xf8 // vpmovzxbd	-0x8(%r15,%rbx), %ymm6
	LONG $0x317dc2c4; WORD $0x1f3c // vpmovzxbd	(%r15,%rbx), %ymm7
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0xed5bfcc5 // vcvtdq2ps	%ymm5, %ymm5
	LONG $0xf65bfcc5 // vcvtdq2ps	%ymm6, %ymm6
	LONG $0xff5bfcc5 // vcvtdq2ps	%ymm7, %ymm7
	LONG $0x04107cc5; BYTE $0x99 // vmovups	(%rcx,%rbx,4), %ymm8
	LONG $0x4c107cc5; WORD $0x2099 // vmovups	0x20(%rcx,%rbx,4), %ymm9
	LONG $0x54107cc5; WORD $0x4099 // vmovups	0x40(%rcx,%rbx,4), %ymm10
	LONG $0x5c107cc5; WORD $0x6099 // vmovups	0x60(%rcx,%rbx,4), %ymm11
	LONG $0xa85d62c4; WORD $0x9a04 // vfmadd213ps	(%rdx,%rbx,4), %ymm4, %ymm8 # ymm8 = (ymm4 * ymm8) + mem
	LONG $0xa85562c4; WORD $0x9a4c; BYTE $0x20 // vfmadd213ps	0x20(%rdx,%rbx,4), %ymm5, %ymm9 # ymm9 = (ymm5 * ymm9) + mem
	LONG $0xa84d62c4; WORD $0x9a54; BYTE $0x40 // vfmadd213ps	0x40(%rdx,%rbx,4), %ymm6, %ymm10 # ymm10 = (ymm6 * ymm10) + mem
	LONG $0xa84562c4; WORD $0x9a5c; BYTE $0x60 // vfmadd213ps	0x60(%rdx,%rbx,4), %ymm7, %ymm11 # ymm11 = (ymm7 * ymm11) + mem
	LONG $0x2410fcc5; BYTE $0x9f // vmovups	(%rdi,%rbx,4), %ymm4
	LONG $0x5c5cc1c4; BYTE $0xe0 // vsubps	%ymm8, %ymm4, %ymm4
	LONG $0x6c10fcc5; WORD $0x209f // vmovups	0x20(%rdi,%rbx,4), %ymm5
	LONG $0x5c54c1c4; BYTE $0xe9 // vsubps	%ymm9, %ymm5, %ymm5
	LONG $0x7410fcc5; WORD $0x409f // vmovups	0x40(%rdi,%rbx,4), %ymm6
	LONG $0x5c4cc1c4; BYTE $0xf2 // vsubps	%ymm10, %ymm6, %ymm6
	LONG $0x7c10fcc5; WORD $0x609f // vmovups	0x60(%rdi,%rbx,4), %ymm7
	LONG $0x5c44c1c4; BYTE $0xfb // vsubps	%ymm11, %ymm7, %ymm7
	LONG $0xb85de2c4; BYTE $0xc4 // vfmadd231ps	%ymm4, %ymm4, %ymm0 # ymm0 = (ymm4 * ymm4) + ymm0
	LONG $0xb855e2c4; BYTE $0xcd // vfmadd231ps	%ymm5, %ymm5, %ymm1 # ymm1 = (ymm5 * ymm5) + ymm1
	LONG $0xb84de2c4; BYTE $0xd6 // vfmadd231ps	%ymm6, %ymm6, %ymm2 # ymm2 = (ymm6 * ymm6) + ymm2
	LONG $0xb845e2c4; BYTE $0xdf // vfmadd231ps	%ymm7, %ymm7, %ymm3 # ymm3 = (ymm7 * ymm7) + ymm3
	LONG $0x20c38348 // addq	$0x20, %rbx
	WORD $0x394c; BYTE $0xdb // cmpq	%r11, %rbx
	JLE LBB1_4
	WORD $0x894c; BYTE $0xcb // movq	%r9, %rbx
	WORD $0x394d; BYTE $0xd1 // cmpq	%r10, %r9
	JGT LBB1_7
LBB1_6:
	LONG $0x317dc2c4; WORD $0x1d64; BYTE $0x00 // vpmovzxbd	(%r13,%rbx), %ymm4
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0x2c10fcc5; BYTE $0x99 // vmovups	(%rcx,%rbx,4), %ymm5
	LONG $0xa85de2c4; WORD $0x9a2c // vfmadd213ps	(%rdx,%rbx,4), %ymm4, %ymm5 # ymm5 = (ymm4 * ymm5) + mem
	LONG $0x2410fcc5; BYTE $0x9f // vmovups	(%rdi,%rbx,4), %ymm4
	LONG $0xe55cdcc5 // vsubps	%ymm5, %ymm4, %ymm4
	LONG $0xb85de2c4; BYTE $0xc4 // vfmadd231ps	%ymm4, %ymm4, %ymm0 # ymm0 = (ymm4 * ymm4) + ymm0
	LONG $0x08c38348 // addq	$0x8, %rbx
	WORD $0x394c; BYTE $0xd3 // cmpq	%r10, %rbx
	JLE LBB1_6
LBB1_7:
	LONG $0xc058f4c5 // vaddps	%ymm0, %ymm1, %ymm0
	LONG $0xca58e4c5 // vaddps	%ymm2, %ymm3, %ymm1
	LONG $0xc058f4c5 // vaddps	%ymm0, %ymm1, %ymm0
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$0x1, %ymm0, %xmm1
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xc87cfbc5 // vhaddps	%xmm0, %xmm0, %xmm1
	WORD $0x3949; BYTE $0xd8 // cmpq	%rbx, %r8
	JLE LBB1_8
	WORD $0x8944; BYTE $0xc0 // movl	%r8d, %eax
	WORD $0xd829 // subl	%ebx, %eax
	WORD $0x8949; BYTE $0xde // movq	%rbx, %r14
	WORD $0x01a8 // testb	$0x1, %al
	JE LBB1_43
	LONG $0x1d44b60f; BYTE $0x00 // movzbl	(%rbp,%rbx), %eax
	LONG $0xc02a9ac5 // vcvtsi2ss	%eax, %xmm12, %xmm0
	LONG $0x1410fac5; BYTE $0x99 // vmovss	(%rcx,%rbx,4), %xmm2
	LONG $0xa979e2c4; WORD $0x9a14 // vfmadd213ss	(%rdx,%rbx,4), %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + mem
	LONG $0x0410fac5; BYTE $0x9f // vmovss	(%rdi,%rbx,4), %xmm0
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x01738d4c // leaq	0x1(%rbx), %r14
	LONG $0xc828f8c5 // vmovaps	%xmm0, %xmm1
LBB1_43:
	LONG $0x245c3b48; BYTE $0x08 // cmpq	0x8(%rsp), %rbx
	JE LBB1_46
	LONG $0xc128f8c5 // vmovaps	%xmm1, %xmm0
LBB1_45:
	LONG $0x44b60f43; WORD $0x0035 // movzbl	(%r13,%r14), %eax
	LONG $0xc82a9ac5 // vcvtsi2ss	%eax, %xmm12, %xmm1
	LONG $0x107aa1c4; WORD $0xb114 // vmovss	(%rcx,%r14,4), %xmm2
	LONG $0x107aa1c4; WORD $0xb15c; BYTE $0x04 // vmovss	0x4(%rcx,%r14,4), %xmm3
	LONG $0xa971a2c4; WORD $0xb214 // vfmadd213ss	(%rdx,%r14,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0xb70c // vmovss	(%rdi,%r14,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0x44b60f43; WORD $0x0135 // movzbl	0x1(%r13,%r14), %eax
	LONG $0xd02a9ac5 // vcvtsi2ss	%eax, %xmm12, %xmm2
	LONG $0xa971e2c4; BYTE $0xc8 // vfmadd213ss	%xmm0, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm0
	LONG $0x107aa1c4; WORD $0xb744; BYTE $0x04 // vmovss	0x4(%rdi,%r14,4), %xmm0
	LONG $0xa961a2c4; WORD $0xb254; BYTE $0x04 // vfmadd213ss	0x4(%rdx,%r14,4), %xmm3, %xmm2 # xmm2 = (xmm3 * xmm2) + mem
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x02c68349 // addq	$0x2, %r14
	WORD $0x394d; BYTE $0xf0 // cmpq	%r14, %r8
	JNE LBB1_45
	JMP LBB1_46
LBB1_10:
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	LONG $0xf8e38349 // andq	$-0x8, %r11
	LONG $0x085b8d49 // leaq	0x8(%r11), %rbx
	WORD $0x894d; BYTE $0xd6 // movq	%r10, %r14
	LONG $0x03eec149 // shrq	$0x3, %r14
	WORD $0xff49; BYTE $0xc6 // incq	%r14
	LONG $0xf7408d49 // leaq	-0x9(%r8), %rax
	LONG $0x24448948; BYTE $0x08 // movq	%rax, 0x8(%rsp)
	LONG $0xfee68349 // andq	$-0x2, %r14
	LONG $0x09438d49 // leaq	0x9(%r11), %rax
	LONG $0x24448948; BYTE $0x10 // movq	%rax, 0x10(%rsp)
	LONG $0x086e8d4c // leaq	0x8(%rsi), %r13
	LONG $0x016e8d48 // leaq	0x1(%rsi), %rbp
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
	JMP LBB1_11
LBB1_39:
	LONG $0x117a81c4; WORD $0xb904 // vmovss	%xmm0, (%r9,%r15,4)
	WORD $0xff49; BYTE $0xc7 // incq	%r15
	WORD $0x014d; BYTE $0xc5 // addq	%r8, %r13
	WORD $0x014c; BYTE $0xc5 // addq	%r8, %rbp
	LONG $0x243c3b4c // cmpq	(%rsp), %r15
	JE LBB1_40
LBB1_11:
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0x08fa8349 // cmpq	$0x8, %r10
	JCC LBB1_31
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0x244c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r9
	JMP LBB1_33
LBB1_31:
	WORD $0x894d; BYTE $0xf4 // movq	%r14, %r12
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0x244c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r9
LBB1_32:
	LONG $0x317dc2c4; WORD $0x054c; BYTE $0xf8 // vpmovzxbd	-0x8(%r13,%rax), %ymm1
	LONG $0xc95bfcc5 // vcvtdq2ps	%ymm1, %ymm1
	LONG $0x1410fcc5; BYTE $0x81 // vmovups	(%rcx,%rax,4), %ymm2
	LONG $0x5c10fcc5; WORD $0x2081 // vmovups	0x20(%rcx,%rax,4), %ymm3
	LONG $0xa875e2c4; WORD $0x8214 // vfmadd213ps	(%rdx,%rax,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	LONG $0x0c10fcc5; BYTE $0x87 // vmovups	(%rdi,%rax,4), %ymm1
	LONG $0xca5cf4c5 // vsubps	%ymm2, %ymm1, %ymm1
	LONG $0xa875e2c4; BYTE $0xc8 // vfmadd213ps	%ymm0, %ymm1, %ymm1 # ymm1 = (ymm1 * ymm1) + ymm0
	LONG $0x317dc2c4; WORD $0x0544; BYTE $0x00 // vpmovzxbd	(%r13,%rax), %ymm0
	LONG $0x5410fcc5; WORD $0x2087 // vmovups	0x20(%rdi,%rax,4), %ymm2
	LONG $0xc05bfcc5 // vcvtdq2ps	%ymm0, %ymm0
	LONG $0xa865e2c4; WORD $0x8244; BYTE $0x20 // vfmadd213ps	0x20(%rdx,%rax,4), %ymm3, %ymm0 # ymm0 = (ymm3 * ymm0) + mem
	LONG $0xc05cecc5 // vsubps	%ymm0, %ymm2, %ymm0
	LONG $0xa87de2c4; BYTE $0xc1 // vfmadd213ps	%ymm1, %ymm0, %ymm0 # ymm0 = (ymm0 * ymm0) + ymm1
	LONG $0x10c08348 // addq	$0x10, %rax
	LONG $0xfec48349 // addq	$-0x2, %r12
	JNE LBB1_32
LBB1_33:
	WORD $0x894d; BYTE $0xfc // movq	%r15, %r12
	LONG $0xe0af0f4d // imulq	%r8, %r12
	WORD $0x0149; BYTE $0xf4 // addq	%rsi, %r12
	LONG $0x08c2f641 // testb	$0x8, %r10b
	JNE LBB1_35
	LONG $0x317dc2c4; WORD $0x040c // vpmovzxbd	(%r12,%rax), %ymm1
	LONG $0xc95bfcc5 // vcvtdq2ps	%ymm1, %ymm1
	LONG $0x1410fcc5; BYTE $0x81 // vmovups	(%rcx,%rax,4), %ymm2
	LONG $0xa875e2c4; WORD $0x8214 // vfmadd213ps	(%rdx,%rax,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	LONG $0x0c10fcc5; BYTE $0x87 // vmovups	(%rdi,%rax,4), %ymm1
	LONG $0xca5cf4c5 // vsubps	%ymm2, %ymm1, %ymm1
	LONG $0xb875e2c4; BYTE $0xc1 // vfmadd231ps	%ymm1, %ymm1, %ymm0 # ymm0 = (ymm1 * ymm1) + ymm0
LBB1_35:
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$0x1, %ymm0, %xmm1
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	LONG $0xc07cfbc5 // vhaddps	%xmm0, %xmm0, %xmm0
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JGE LBB1_39
	WORD $0x8948; BYTE $0xd8 // movq	%rbx, %rax
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JE LBB1_38
	LONG $0x44b60f43; WORD $0x081c // movzbl	0x8(%r12,%r11), %eax
	LONG $0xc82adac5 // vcvtsi2ss	%eax, %xmm4, %xmm1
	LONG $0x107aa1c4; WORD $0x9954; BYTE $0x20 // vmovss	0x20(%rcx,%r11,4), %xmm2
	LONG $0xa971a2c4; WORD $0x9a54; BYTE $0x20 // vfmadd213ss	0x20(%rdx,%r11,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0x9f4c; BYTE $0x20 // vmovss	0x20(%rdi,%r11,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0xb971e2c4; BYTE $0xc1 // vfmadd231ss	%xmm1, %xmm1, %xmm0 # xmm0 = (xmm1 * xmm1) + xmm0
	LONG $0x24448b48; BYTE $0x10 // movq	0x10(%rsp), %rax
LBB1_38:
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB1_39
LBB1_47:
	LONG $0x64b60f44; WORD $0xff05 // movzbl	-0x1(%rbp,%rax), %r12d
	LONG $0x2a5ac1c4; BYTE $0xcc // vcvtsi2ss	%r12d, %xmm4, %xmm1
	LONG $0x1410fac5; BYTE $0x81 // vmovss	(%rcx,%rax,4), %xmm2
	LONG $0x5c10fac5; WORD $0x0481 // vmovss	0x4(%rcx,%rax,4), %xmm3
	LONG $0xa971e2c4; WORD $0x8214 // vfmadd213ss	(%rdx,%rax,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x0c10fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0x64b60f44; WORD $0x0005 // movzbl	(%rbp,%rax), %r12d
	LONG $0x2a5ac1c4; BYTE $0xd4 // vcvtsi2ss	%r12d, %xmm4, %xmm2
	LONG $0xa971e2c4; BYTE $0xc8 // vfmadd213ss	%xmm0, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm0
	LONG $0x4410fac5; WORD $0x0487 // vmovss	0x4(%rdi,%rax,4), %xmm0
	LONG $0xa961e2c4; WORD $0x8254; BYTE $0x04 // vfmadd213ss	0x4(%rdx,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * xmm2) + mem
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x02c08348 // addq	$0x2, %rax
	WORD $0x3949; BYTE $0xc0 // cmpq	%rax, %r8
	JNE LBB1_47
	JMP LBB1_39
LBB1_14:
	LONG $0x24148b48 // movq	(%rsp), %rdx
	WORD $0xd089 // movl	%edx, %eax
	WORD $0xe083; BYTE $0x07 // andl	$0x7, %eax
	LONG $0x08fa8348 // cmpq	$0x8, %rdx
	JCC LBB1_26
	WORD $0xc931 // xorl	%ecx, %ecx
	JMP LBB1_28
LBB1_26:
	QUAD $0xfffffffffff8b948; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %rcx # imm = 0x7FFFFFFFFFFFFFF8
	WORD $0x2148; BYTE $0xca // andq	%rcx, %rdx
	WORD $0xc931 // xorl	%ecx, %ecx
LBB1_27:
	LONG $0x117ac1c4; WORD $0x8904 // vmovss	%xmm0, (%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x04 // vmovss	%xmm0, 0x4(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x08 // vmovss	%xmm0, 0x8(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x0c // vmovss	%xmm0, 0xc(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x10 // vmovss	%xmm0, 0x10(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x14 // vmovss	%xmm0, 0x14(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x18 // vmovss	%xmm0, 0x18(%r9,%rcx,4)
	LONG $0x117ac1c4; WORD $0x8944; BYTE $0x1c // vmovss	%xmm0, 0x1c(%r9,%rcx,4)
	LONG $0x08c18348 // addq	$0x8, %rcx
	WORD $0x3948; BYTE $0xca // cmpq	%rcx, %rdx
	JNE LBB1_27
LBB1_28:
	WORD $0x8548; BYTE $0xc0 // testq	%rax, %rax
	JE LBB1_40
	LONG $0x24548b48; BYTE $0x50 // movq	0x50(%rsp), %rdx
	LONG $0x8a0c8d48 // leaq	(%rdx,%rcx,4), %rcx
	WORD $0xd231 // xorl	%edx, %edx
LBB1_30:
	LONG $0x0411fac5; BYTE $0x91 // vmovss	%xmm0, (%rcx,%rdx,4)
	WORD $0xff48; BYTE $0xc2 // incq	%rdx
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	JNE LBB1_30
LBB1_40:
	LONG $0x18c48348 // addq	$0x18, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

