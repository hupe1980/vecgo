// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT ·sq8L2BatchAvx512(SB), NOSPLIT, $0-56
	MOVQ query+0(FP), DI
	MOVQ codes+8(FP), SI
	MOVQ scales+16(FP), DX
	MOVQ biases+24(FP), CX
	MOVQ dim+32(FP), R8
	MOVQ n+40(FP), R9
	PUSHQ out+48(FP)
	PUSHQ $0

	WORD $0x854d; BYTE $0xc9 // testq	%r9, %r9
	JLE LBB0_33
	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x18ec8348 // subq	$0x18, %rsp
	LONG $0x247c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r15
	LONG $0x10f88349 // cmpq	$0x10, %r8
	JGE LBB0_2
	WORD $0x854d; BYTE $0xc0 // testq	%r8, %r8
	JLE LBB0_6
	QUAD $0xfffffffffff8ba49; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %r10 # imm = 0x7FFFFFFFFFFFFFF8
	LONG $0x06ca8349 // orq	$0x6, %r10
	WORD $0x214d; BYTE $0xc2 // andq	%r8, %r10
	LONG $0x015e8d4c // leaq	0x1(%rsi), %r11
	WORD $0xdb31 // xorl	%ebx, %ebx
	JMP LBB0_9
LBB0_13:
	LONG $0x117ac1c4; WORD $0x9f14 // vmovss	%xmm2, (%r15,%rbx,4)
	WORD $0xff48; BYTE $0xc3 // incq	%rbx
	WORD $0x014d; BYTE $0xc3 // addq	%r8, %r11
	WORD $0x394c; BYTE $0xcb // cmpq	%r9, %rbx
	JE LBB0_32
LBB0_9:
	LONG $0x0410fac5; BYTE $0x9a // vmovss	(%rdx,%rbx,4), %xmm0
	LONG $0x0c10fac5; BYTE $0x99 // vmovss	(%rcx,%rbx,4), %xmm1
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	LONG $0x01f88349 // cmpq	$0x1, %r8
	JE LBB0_11
LBB0_10:
	LONG $0x44be0f43; WORD $0xff33 // movsbl	-0x1(%r11,%r14), %eax
	LONG $0xd82ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0xb724 // vmovss	(%rdi,%r14,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0xb764; BYTE $0x04 // vmovss	0x4(%rdi,%r14,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x04be0f43; BYTE $0x33 // movsbl	(%r11,%r14), %eax
	LONG $0xd02ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c68349 // addq	$0x2, %r14
	WORD $0x394d; BYTE $0xf2 // cmpq	%r14, %r10
	JNE LBB0_10
LBB0_11:
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JE LBB0_13
	WORD $0x8948; BYTE $0xd8 // movq	%rbx, %rax
	LONG $0xc0af0f49 // imulq	%r8, %rax
	WORD $0x0148; BYTE $0xf0 // addq	%rsi, %rax
	LONG $0x04be0f42; BYTE $0x30 // movsbl	(%rax,%r14), %eax
	LONG $0xd82ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm3, %xmm0 # xmm0 = (xmm3 * xmm0) + xmm1
	LONG $0x107aa1c4; WORD $0xb70c // vmovss	(%rdi,%r14,4), %xmm1
	LONG $0xc05cf2c5 // vsubss	%xmm0, %xmm1, %xmm0
	LONG $0xb979e2c4; BYTE $0xd0 // vfmadd231ss	%xmm0, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm0) + xmm2
	JMP LBB0_13
LBB0_2:
	LONG $0xf0508d4d // leaq	-0x10(%r8), %r10
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	LONG $0xf0e38349 // andq	$-0x10, %r11
	LONG $0x105b8d49 // leaq	0x10(%r11), %rbx
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	LONG $0x04e8c148 // shrq	$0x4, %rax
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	LONG $0xef708d4d // leaq	-0x11(%r8), %r14
	LONG $0x2474894c; BYTE $0x08 // movq	%r14, 0x8(%rsp)
	LONG $0xfee08348 // andq	$-0x2, %rax
	LONG $0x24448948; BYTE $0x10 // movq	%rax, 0x10(%rsp)
	LONG $0x11438d49 // leaq	0x11(%r11), %rax
	LONG $0x24048948 // movq	%rax, (%rsp)
	LONG $0x106e8d4c // leaq	0x10(%rsi), %r13
	LONG $0x016e8d48 // leaq	0x1(%rsi), %rbp
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB0_3
LBB0_24:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_31:
	WORD $0x894d; BYTE $0xf7 // movq	%r14, %r15
	LONG $0x117ac1c4; WORD $0x8614 // vmovss	%xmm2, (%r14,%rax,4)
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	WORD $0x014d; BYTE $0xc5 // addq	%r8, %r13
	WORD $0x014c; BYTE $0xc5 // addq	%r8, %rbp
	WORD $0x394c; BYTE $0xc8 // cmpq	%r9, %rax
	JE LBB0_32
LBB0_3:
	WORD $0x894d; BYTE $0xfe // movq	%r15, %r14
	LONG $0x487df262; WORD $0x0418; BYTE $0x82 // vbroadcastss	(%rdx,%rax,4), %zmm0
	LONG $0x487df262; WORD $0x0c18; BYTE $0x81 // vbroadcastss	(%rcx,%rax,4), %zmm1
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0x10fa8349 // cmpq	$0x10, %r10
	JAE LBB0_19
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
	JMP LBB0_21
LBB0_19:
	LONG $0x24648b4c; BYTE $0x10 // movq	0x10(%rsp), %r12
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
LBB0_20:
	QUAD $0xff3d5c21487d9262 // vpmovsxbd	-0x10(%r13,%r15), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0xbf // vmovups	(%rdi,%r15,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	QUAD $0x01bf6410487cb162 // vmovups	0x40(%rdi,%r15,4), %zmm4
	LONG $0x4865f262; WORD $0xdaa8 // vfmadd213ps	%zmm2, %zmm3, %zmm3 # zmm3 = (zmm3 * zmm3) + zmm2
	QUAD $0x003d5421487d9262 // vpmovsxbd	(%r13,%r15), %zmm2
	LONG $0x487cf162; WORD $0xd25b // vcvtdq2ps	%zmm2, %zmm2
	LONG $0x487df262; WORD $0xd1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm2 # zmm2 = (zmm0 * zmm2) + zmm1
	LONG $0x485cf162; WORD $0xd25c // vsubps	%zmm2, %zmm4, %zmm2
	LONG $0x486df262; WORD $0xd3a8 // vfmadd213ps	%zmm3, %zmm2, %zmm2 # zmm2 = (zmm2 * zmm2) + zmm3
	LONG $0x20c78349 // addq	$0x20, %r15
	LONG $0xfec48349 // addq	$-0x2, %r12
	JNE LBB0_20
LBB0_21:
	WORD $0x8949; BYTE $0xc4 // movq	%rax, %r12
	LONG $0xe0af0f4d // imulq	%r8, %r12
	WORD $0x0149; BYTE $0xf4 // addq	%rsi, %r12
	LONG $0x10c2f641 // testb	$0x10, %r10b
	JNE LBB0_23
	LONG $0x487d9262; WORD $0x1c21; BYTE $0x3c // vpmovsxbd	(%r12,%r15), %zmm3
	LONG $0x487cf162; WORD $0xdb5b // vcvtdq2ps	%zmm3, %zmm3
	LONG $0x487df262; WORD $0xd9a8 // vfmadd213ps	%zmm1, %zmm0, %zmm3 # zmm3 = (zmm0 * zmm3) + zmm1
	LONG $0x487cb162; WORD $0x2410; BYTE $0xbf // vmovups	(%rdi,%r15,4), %zmm4
	LONG $0x485cf162; WORD $0xdb5c // vsubps	%zmm3, %zmm4, %zmm3
	LONG $0x4865f262; WORD $0xd3b8 // vfmadd231ps	%zmm3, %zmm3, %zmm2 # zmm2 = (zmm3 * zmm3) + zmm2
LBB0_23:
	LONG $0x48fdf362; WORD $0xd31b; BYTE $0x01 // vextractf64x4	$0x1, %zmm2, %ymm3
	LONG $0x486cf162; WORD $0xd358 // vaddps	%zmm3, %zmm2, %zmm2
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xdac6e9c5; BYTE $0x01 // vshufpd	$0x1, %xmm2, %xmm2, %xmm3 # xmm3 = xmm2[1,0]
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xda16fac5 // vmovshdup	%xmm2, %xmm3    # xmm3 = xmm2[1,1,3,3]
	LONG $0xdb58eac5 // vaddss	%xmm3, %xmm2, %xmm3
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JGE LBB0_24
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JNE LBB0_27
	WORD $0x8949; BYTE $0xdf // movq	%rbx, %r15
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB0_31
	JMP LBB0_29
LBB0_27:
	LONG $0x7cbe0f47; WORD $0x101c // movsbl	0x10(%r12,%r11), %r15d
	LONG $0x2a52c1c4; BYTE $0xd7 // vcvtsi2ss	%r15d, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x107aa1c4; WORD $0x9f64; BYTE $0x40 // vmovss	0x40(%rdi,%r11,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	LONG $0x243c8b4c // movq	(%rsp), %r15
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB0_31
LBB0_29:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_30:
	LONG $0x64be0f46; WORD $0xff3d // movsbl	-0x1(%rbp,%r15), %r12d
	LONG $0x2a52c1c4; BYTE $0xdc // vcvtsi2ss	%r12d, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0xbf24 // vmovss	(%rdi,%r15,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0xbf64; BYTE $0x04 // vmovss	0x4(%rdi,%r15,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x64be0f46; WORD $0x003d // movsbl	(%rbp,%r15), %r12d
	LONG $0x2a52c1c4; BYTE $0xd4 // vcvtsi2ss	%r12d, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c78349 // addq	$0x2, %r15
	WORD $0x394d; BYTE $0xf8 // cmpq	%r15, %r8
	JNE LBB0_30
	JMP LBB0_31
LBB0_6:
	WORD $0x8944; BYTE $0xc9 // movl	%r9d, %ecx
	WORD $0xe183; BYTE $0x07 // andl	$0x7, %ecx
	LONG $0x08f98349 // cmpq	$0x8, %r9
	JAE LBB0_14
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB0_16
LBB0_14:
	QUAD $0xfffffffffff8b848; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %rax # imm = 0x7FFFFFFFFFFFFFF8
	WORD $0x2149; BYTE $0xc1 // andq	%rax, %r9
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
LBB0_15:
	LONG $0x117cc1c4; WORD $0x8704 // vmovups	%ymm0, (%r15,%rax,4)
	LONG $0x08c08348 // addq	$0x8, %rax
	WORD $0x3949; BYTE $0xc1 // cmpq	%rax, %r9
	JNE LBB0_15
LBB0_16:
	WORD $0x8548; BYTE $0xc9 // testq	%rcx, %rcx
	JE LBB0_32
	LONG $0x87048d49 // leaq	(%r15,%rax,4), %rax
	WORD $0xd231 // xorl	%edx, %edx
LBB0_18:
	LONG $0x009004c7; WORD $0x0000; BYTE $0x00 // movl	$0x0, (%rax,%rdx,4)
	WORD $0xff48; BYTE $0xc2 // incq	%rdx
	WORD $0x3948; BYTE $0xd1 // cmpq	%rdx, %rcx
	JNE LBB0_18
LBB0_32:
	LONG $0x18c48348 // addq	$0x18, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
LBB0_33:
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	POPQ AX
	POPQ AX
	RET

TEXT ·sq8uL2BatchPerDimensionAvx512(SB), NOSPLIT, $0-56
	MOVQ query+0(FP), DI
	MOVQ codes+8(FP), SI
	MOVQ mins+16(FP), DX
	MOVQ invScales+24(FP), CX
	MOVQ dim+32(FP), R8
	MOVQ n+40(FP), R9
	PUSHQ out+48(FP)
	PUSHQ $0

	WORD $0x854d; BYTE $0xc9 // testq	%r9, %r9
	JLE LBB1_33
	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x18ec8348 // subq	$0x18, %rsp
	LONG $0x247c8b4c; BYTE $0x50 // movq	0x50(%rsp), %r15
	LONG $0x10f88349 // cmpq	$0x10, %r8
	JGE LBB1_2
	WORD $0x854d; BYTE $0xc0 // testq	%r8, %r8
	JLE LBB1_6
	QUAD $0xfffffffffff8ba49; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %r10 # imm = 0x7FFFFFFFFFFFFFF8
	LONG $0x06ca8349 // orq	$0x6, %r10
	WORD $0x214d; BYTE $0xc2 // andq	%r8, %r10
	LONG $0x015e8d4c // leaq	0x1(%rsi), %r11
	WORD $0xdb31 // xorl	%ebx, %ebx
	JMP LBB1_9
LBB1_13:
	LONG $0x117ac1c4; WORD $0x9f04 // vmovss	%xmm0, (%r15,%rbx,4)
	WORD $0xff48; BYTE $0xc3 // incq	%rbx
	WORD $0x014d; BYTE $0xc3 // addq	%r8, %r11
	WORD $0x394c; BYTE $0xcb // cmpq	%r9, %rbx
	JE LBB1_32
LBB1_9:
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	LONG $0x01f88349 // cmpq	$0x1, %r8
	JE LBB1_11
LBB1_10:
	LONG $0x44b60f43; WORD $0xff33 // movzbl	-0x1(%r11,%r14), %eax
	LONG $0xc82adac5 // vcvtsi2ss	%eax, %xmm4, %xmm1
	LONG $0x107aa1c4; WORD $0xb114 // vmovss	(%rcx,%r14,4), %xmm2
	LONG $0x107aa1c4; WORD $0xb15c; BYTE $0x04 // vmovss	0x4(%rcx,%r14,4), %xmm3
	LONG $0xa971a2c4; WORD $0xb214 // vfmadd213ss	(%rdx,%r14,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0xb70c // vmovss	(%rdi,%r14,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0x04b60f43; BYTE $0x33 // movzbl	(%r11,%r14), %eax
	LONG $0xd02adac5 // vcvtsi2ss	%eax, %xmm4, %xmm2
	LONG $0xa971e2c4; BYTE $0xc8 // vfmadd213ss	%xmm0, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm0
	LONG $0x107aa1c4; WORD $0xb744; BYTE $0x04 // vmovss	0x4(%rdi,%r14,4), %xmm0
	LONG $0xa961a2c4; WORD $0xb254; BYTE $0x04 // vfmadd213ss	0x4(%rdx,%r14,4), %xmm3, %xmm2 # xmm2 = (xmm3 * xmm2) + mem
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x02c68349 // addq	$0x2, %r14
	WORD $0x394d; BYTE $0xf2 // cmpq	%r14, %r10
	JNE LBB1_10
LBB1_11:
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JE LBB1_13
	WORD $0x8948; BYTE $0xd8 // movq	%rbx, %rax
	LONG $0xc0af0f49 // imulq	%r8, %rax
	WORD $0x0148; BYTE $0xf0 // addq	%rsi, %rax
	LONG $0x04b60f42; BYTE $0x30 // movzbl	(%rax,%r14), %eax
	LONG $0xc82adac5 // vcvtsi2ss	%eax, %xmm4, %xmm1
	LONG $0x107aa1c4; WORD $0xb114 // vmovss	(%rcx,%r14,4), %xmm2
	LONG $0xa971a2c4; WORD $0xb214 // vfmadd213ss	(%rdx,%r14,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0xb70c // vmovss	(%rdi,%r14,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0xb971e2c4; BYTE $0xc1 // vfmadd231ss	%xmm1, %xmm1, %xmm0 # xmm0 = (xmm1 * xmm1) + xmm0
	JMP LBB1_13
LBB1_2:
	LONG $0xf0508d4d // leaq	-0x10(%r8), %r10
	WORD $0x894d; BYTE $0xd3 // movq	%r10, %r11
	LONG $0xf0e38349 // andq	$-0x10, %r11
	LONG $0x105b8d49 // leaq	0x10(%r11), %rbx
	WORD $0x894c; BYTE $0xd0 // movq	%r10, %rax
	LONG $0x04e8c148 // shrq	$0x4, %rax
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	LONG $0xef708d4d // leaq	-0x11(%r8), %r14
	LONG $0x2474894c; BYTE $0x08 // movq	%r14, 0x8(%rsp)
	LONG $0xfee08348 // andq	$-0x2, %rax
	LONG $0x24448948; BYTE $0x10 // movq	%rax, 0x10(%rsp)
	LONG $0x11438d49 // leaq	0x11(%r11), %rax
	LONG $0x24048948 // movq	%rax, (%rsp)
	LONG $0x106e8d4c // leaq	0x10(%rsi), %r13
	LONG $0x016e8d48 // leaq	0x1(%rsi), %rbp
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB1_3
LBB1_24:
	LONG $0xc128f8c5 // vmovaps	%xmm1, %xmm0
LBB1_31:
	WORD $0x894d; BYTE $0xf7 // movq	%r14, %r15
	LONG $0x117ac1c4; WORD $0x8604 // vmovss	%xmm0, (%r14,%rax,4)
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	WORD $0x014d; BYTE $0xc5 // addq	%r8, %r13
	WORD $0x014c; BYTE $0xc5 // addq	%r8, %rbp
	WORD $0x394c; BYTE $0xc8 // cmpq	%r9, %rax
	JE LBB1_32
LBB1_3:
	WORD $0x894d; BYTE $0xfe // movq	%r15, %r14
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0x10fa8349 // cmpq	$0x10, %r10
	JAE LBB1_19
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
	JMP LBB1_21
LBB1_19:
	LONG $0x24648b4c; BYTE $0x10 // movq	0x10(%rsp), %r12
	WORD $0x3145; BYTE $0xff // xorl	%r15d, %r15d
LBB1_20:
	QUAD $0xff3d4c31487d9262 // vpmovzxbd	-0x10(%r13,%r15), %zmm1
	LONG $0x487cf162; WORD $0xc95b // vcvtdq2ps	%zmm1, %zmm1
	LONG $0x487cb162; WORD $0x1410; BYTE $0xb9 // vmovups	(%rcx,%r15,4), %zmm2
	QUAD $0x01b95c10487cb162 // vmovups	0x40(%rcx,%r15,4), %zmm3
	LONG $0x4875b262; WORD $0x14a8; BYTE $0xba // vfmadd213ps	(%rdx,%r15,4), %zmm1, %zmm2 # zmm2 = (zmm1 * zmm2) + mem
	LONG $0x487cb162; WORD $0x0c10; BYTE $0xbf // vmovups	(%rdi,%r15,4), %zmm1
	LONG $0x4874f162; WORD $0xca5c // vsubps	%zmm2, %zmm1, %zmm1
	LONG $0x4875f262; WORD $0xc8a8 // vfmadd213ps	%zmm0, %zmm1, %zmm1 # zmm1 = (zmm1 * zmm1) + zmm0
	QUAD $0x003d4431487d9262 // vpmovzxbd	(%r13,%r15), %zmm0
	QUAD $0x01bf5410487cb162 // vmovups	0x40(%rdi,%r15,4), %zmm2
	LONG $0x487cf162; WORD $0xc05b // vcvtdq2ps	%zmm0, %zmm0
	QUAD $0x01ba44a84865b262 // vfmadd213ps	0x40(%rdx,%r15,4), %zmm3, %zmm0 # zmm0 = (zmm3 * zmm0) + mem
	LONG $0x486cf162; WORD $0xc05c // vsubps	%zmm0, %zmm2, %zmm0
	LONG $0x487df262; WORD $0xc1a8 // vfmadd213ps	%zmm1, %zmm0, %zmm0 # zmm0 = (zmm0 * zmm0) + zmm1
	LONG $0x20c78349 // addq	$0x20, %r15
	LONG $0xfec48349 // addq	$-0x2, %r12
	JNE LBB1_20
LBB1_21:
	WORD $0x8949; BYTE $0xc4 // movq	%rax, %r12
	LONG $0xe0af0f4d // imulq	%r8, %r12
	WORD $0x0149; BYTE $0xf4 // addq	%rsi, %r12
	LONG $0x10c2f641 // testb	$0x10, %r10b
	JNE LBB1_23
	LONG $0x487d9262; WORD $0x0c31; BYTE $0x3c // vpmovzxbd	(%r12,%r15), %zmm1
	LONG $0x487cf162; WORD $0xc95b // vcvtdq2ps	%zmm1, %zmm1
	LONG $0x487cb162; WORD $0x1410; BYTE $0xb9 // vmovups	(%rcx,%r15,4), %zmm2
	LONG $0x4875b262; WORD $0x14a8; BYTE $0xba // vfmadd213ps	(%rdx,%r15,4), %zmm1, %zmm2 # zmm2 = (zmm1 * zmm2) + mem
	LONG $0x487cb162; WORD $0x0c10; BYTE $0xbf // vmovups	(%rdi,%r15,4), %zmm1
	LONG $0x4874f162; WORD $0xca5c // vsubps	%zmm2, %zmm1, %zmm1
	LONG $0x4875f262; WORD $0xc1b8 // vfmadd231ps	%zmm1, %zmm1, %zmm0 # zmm0 = (zmm1 * zmm1) + zmm0
LBB1_23:
	LONG $0x48fdf362; WORD $0xc11b; BYTE $0x01 // vextractf64x4	$0x1, %zmm0, %ymm1
	LONG $0x487cf162; WORD $0xc158 // vaddps	%zmm1, %zmm0, %zmm0
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$0x1, %ymm0, %xmm1
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01 // vshufpd	$0x1, %xmm0, %xmm0, %xmm1 # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5 // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5 // vmovshdup	%xmm0, %xmm1    # xmm1 = xmm0[1,1,3,3]
	LONG $0xc958fac5 // vaddss	%xmm1, %xmm0, %xmm1
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JGE LBB1_24
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JNE LBB1_27
	WORD $0x8949; BYTE $0xdf // movq	%rbx, %r15
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB1_31
	JMP LBB1_29
LBB1_27:
	LONG $0x7cb60f47; WORD $0x101c // movzbl	0x10(%r12,%r11), %r15d
	LONG $0x2a5ac1c4; BYTE $0xc7 // vcvtsi2ss	%r15d, %xmm4, %xmm0
	LONG $0x107aa1c4; WORD $0x9954; BYTE $0x40 // vmovss	0x40(%rcx,%r11,4), %xmm2
	LONG $0xa979a2c4; WORD $0x9a54; BYTE $0x40 // vfmadd213ss	0x40(%rdx,%r11,4), %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0x9f44; BYTE $0x40 // vmovss	0x40(%rdi,%r11,4), %xmm0
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0xc828f8c5 // vmovaps	%xmm0, %xmm1
	LONG $0x243c8b4c // movq	(%rsp), %r15
	LONG $0x245c394c; BYTE $0x08 // cmpq	%r11, 0x8(%rsp)
	JE LBB1_31
LBB1_29:
	LONG $0xc128f8c5 // vmovaps	%xmm1, %xmm0
LBB1_30:
	LONG $0x64b60f46; WORD $0xff3d // movzbl	-0x1(%rbp,%r15), %r12d
	LONG $0x2a5ac1c4; BYTE $0xcc // vcvtsi2ss	%r12d, %xmm4, %xmm1
	LONG $0x107aa1c4; WORD $0xb914 // vmovss	(%rcx,%r15,4), %xmm2
	LONG $0x107aa1c4; WORD $0xb95c; BYTE $0x04 // vmovss	0x4(%rcx,%r15,4), %xmm3
	LONG $0xa971a2c4; WORD $0xba14 // vfmadd213ss	(%rdx,%r15,4), %xmm1, %xmm2 # xmm2 = (xmm1 * xmm2) + mem
	LONG $0x107aa1c4; WORD $0xbf0c // vmovss	(%rdi,%r15,4), %xmm1
	LONG $0xca5cf2c5 // vsubss	%xmm2, %xmm1, %xmm1
	LONG $0x64b60f46; WORD $0x003d // movzbl	(%rbp,%r15), %r12d
	LONG $0x2a5ac1c4; BYTE $0xd4 // vcvtsi2ss	%r12d, %xmm4, %xmm2
	LONG $0xa971e2c4; BYTE $0xc8 // vfmadd213ss	%xmm0, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm0
	LONG $0x107aa1c4; WORD $0xbf44; BYTE $0x04 // vmovss	0x4(%rdi,%r15,4), %xmm0
	LONG $0xa961a2c4; WORD $0xba54; BYTE $0x04 // vfmadd213ss	0x4(%rdx,%r15,4), %xmm3, %xmm2 # xmm2 = (xmm3 * xmm2) + mem
	LONG $0xc25cfac5 // vsubss	%xmm2, %xmm0, %xmm0
	LONG $0xa979e2c4; BYTE $0xc1 // vfmadd213ss	%xmm1, %xmm0, %xmm0 # xmm0 = (xmm0 * xmm0) + xmm1
	LONG $0x02c78349 // addq	$0x2, %r15
	WORD $0x394d; BYTE $0xf8 // cmpq	%r15, %r8
	JNE LBB1_30
	JMP LBB1_31
LBB1_6:
	WORD $0x8944; BYTE $0xc9 // movl	%r9d, %ecx
	WORD $0xe183; BYTE $0x07 // andl	$0x7, %ecx
	LONG $0x08f98349 // cmpq	$0x8, %r9
	JAE LBB1_14
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB1_16
LBB1_14:
	QUAD $0xfffffffffff8b848; WORD $0x7fff // movabsq	$0x7ffffffffffffff8, %rax # imm = 0x7FFFFFFFFFFFFFF8
	WORD $0x2149; BYTE $0xc1 // andq	%rax, %r9
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0
LBB1_15:
	LONG $0x117cc1c4; WORD $0x8704 // vmovups	%ymm0, (%r15,%rax,4)
	LONG $0x08c08348 // addq	$0x8, %rax
	WORD $0x3949; BYTE $0xc1 // cmpq	%rax, %r9
	JNE LBB1_15
LBB1_16:
	WORD $0x8548; BYTE $0xc9 // testq	%rcx, %rcx
	JE LBB1_32
	LONG $0x87048d49 // leaq	(%r15,%rax,4), %rax
	WORD $0xd231 // xorl	%edx, %edx
LBB1_18:
	LONG $0x009004c7; WORD $0x0000; BYTE $0x00 // movl	$0x0, (%rax,%rdx,4)
	WORD $0xff48; BYTE $0xc2 // incq	%rdx
	WORD $0x3948; BYTE $0xd1 // cmpq	%rdx, %rcx
	JNE LBB1_18
LBB1_32:
	LONG $0x18c48348 // addq	$0x18, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
LBB1_33:
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	POPQ AX
	POPQ AX
	RET

