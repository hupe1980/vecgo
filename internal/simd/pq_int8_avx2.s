// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT ·buildDistanceTableInt8Avx2(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ codebook+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ out+40(FP), R9

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x187de2c4; BYTE $0x01 // vbroadcastss	(%rcx), %ymm0
	LONG $0x187dc2c4; BYTE $0x08 // vbroadcastss	(%r8), %ymm1
	LONG $0x08fa8348 // cmpq	$0x8, %rdx
	JGE LBB1_1
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	JLE LBB1_5
	LONG $0x1f10fac5 // vmovss	(%rdi), %xmm3
	LONG $0x06c68348 // addq	$0x6, %rsi
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB1_8
LBB1_15:
	LONG $0x117ac1c4; WORD $0x8124 // vmovss	%xmm4, (%r9,%rax,4)
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	WORD $0x0148; BYTE $0xd6 // addq	%rdx, %rsi
	LONG $0x01003d48; WORD $0x0000 // cmpq	$0x100, %rax            # imm = 0x100
	JE LBB1_16
LBB1_8:
	LONG $0xfa4ebe0f // movsbl	-0x6(%rsi), %ecx
	LONG $0xe12ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0xe45ce2c5 // vsubss	%xmm4, %xmm3, %xmm4
	LONG $0xa959e2c4; BYTE $0xe2 // vfmadd213ss	%xmm2, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm2
	LONG $0x01fa8348 // cmpq	$0x1, %rdx
	JE LBB1_15
	LONG $0xfb4ebe0f // movsbl	-0x5(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x04 // vmovss	0x4(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x02fa8348 // cmpq	$0x2, %rdx
	JE LBB1_15
	LONG $0xfc4ebe0f // movsbl	-0x4(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x08 // vmovss	0x8(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x03fa8348 // cmpq	$0x3, %rdx
	JE LBB1_15
	LONG $0xfd4ebe0f // movsbl	-0x3(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x0c // vmovss	0xc(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x04fa8348 // cmpq	$0x4, %rdx
	JE LBB1_15
	LONG $0xfe4ebe0f // movsbl	-0x2(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x10 // vmovss	0x10(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x05fa8348 // cmpq	$0x5, %rdx
	JE LBB1_15
	LONG $0xff4ebe0f // movsbl	-0x1(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x14 // vmovss	0x14(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	LONG $0x06fa8348 // cmpq	$0x6, %rdx
	JE LBB1_15
	WORD $0xbe0f; BYTE $0x0e // movsbl	(%rsi), %ecx
	LONG $0xe92ac2c5 // vcvtsi2ss	%ecx, %xmm7, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0x7710fac5; BYTE $0x18 // vmovss	0x18(%rdi), %xmm6
	LONG $0xed5ccac5 // vsubss	%xmm5, %xmm6, %xmm5
	LONG $0xb951e2c4; BYTE $0xe5 // vfmadd231ss	%xmm5, %xmm5, %xmm4 # xmm4 = (xmm5 * xmm5) + xmm4
	JMP LBB1_15
LBB1_1:
	LONG $0xf8428d48 // leaq	-0x8(%rdx), %rax
	WORD $0x8948; BYTE $0xc1 // movq	%rax, %rcx
	LONG $0xf8e18348 // andq	$-0x8, %rcx
	LONG $0x08418d4c // leaq	0x8(%rcx), %r8
	WORD $0x8949; BYTE $0xc2 // movq	%rax, %r10
	LONG $0x03eac149 // shrq	$0x3, %r10
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	LONG $0xf75a8d4c // leaq	-0x9(%rdx), %r11
	LONG $0xfee28349 // andq	$-0x2, %r10
	LONG $0x09598d48 // leaq	0x9(%rcx), %rbx
	LONG $0x08768d4c // leaq	0x8(%rsi), %r14
	LONG $0x017e8d4c // leaq	0x1(%rsi), %r15
	WORD $0x3145; BYTE $0xe4 // xorl	%r12d, %r12d
	JMP LBB1_2
LBB1_22:
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB1_28:
	LONG $0x117a81c4; WORD $0xa114 // vmovss	%xmm2, (%r9,%r12,4)
	WORD $0xff49; BYTE $0xc4 // incq	%r12
	WORD $0x0149; BYTE $0xd6 // addq	%rdx, %r14
	WORD $0x0149; BYTE $0xd7 // addq	%rdx, %r15
	LONG $0x00fc8149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r12            # imm = 0x100
	JE LBB1_16
LBB1_2:
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0x08f88348 // cmpq	$0x8, %rax
	JCC LBB1_17
	WORD $0x3145; BYTE $0xed // xorl	%r13d, %r13d
	JMP LBB1_19
LBB1_17:
	WORD $0x894c; BYTE $0xd5 // movq	%r10, %rbp
	WORD $0x3145; BYTE $0xed // xorl	%r13d, %r13d
LBB1_18:
	LONG $0x217d82c4; WORD $0x2e5c; BYTE $0xf8 // vpmovsxbd	-0x8(%r14,%r13), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0xaf24 // vmovups	(%rdi,%r13,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0x107ca1c4; WORD $0xaf64; BYTE $0x20 // vmovups	0x20(%rdi,%r13,4), %ymm4
	LONG $0xa865e2c4; BYTE $0xda // vfmadd213ps	%ymm2, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm2
	LONG $0x217d82c4; WORD $0x2e14 // vpmovsxbd	(%r14,%r13), %ymm2
	LONG $0xd25bfcc5 // vcvtdq2ps	%ymm2, %ymm2
	LONG $0xa87de2c4; BYTE $0xd1 // vfmadd213ps	%ymm1, %ymm0, %ymm2 # ymm2 = (ymm0 * ymm2) + ymm1
	LONG $0xd25cdcc5 // vsubps	%ymm2, %ymm4, %ymm2
	LONG $0xa86de2c4; BYTE $0xd3 // vfmadd213ps	%ymm3, %ymm2, %ymm2 # ymm2 = (ymm2 * ymm2) + ymm3
	LONG $0x10c58349 // addq	$0x10, %r13
	LONG $0xfec58348 // addq	$-0x2, %rbp
	JNE LBB1_18
LBB1_19:
	WORD $0x894c; BYTE $0xe5 // movq	%r12, %rbp
	LONG $0xeaaf0f48 // imulq	%rdx, %rbp
	WORD $0x0148; BYTE $0xf5 // addq	%rsi, %rbp
	WORD $0x08a8 // testb	$0x8, %al
	JNE LBB1_21
	LONG $0x217da2c4; WORD $0x2d5c; BYTE $0x00 // vpmovsxbd	(%rbp,%r13), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0xaf24 // vmovups	(%rdi,%r13,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0xb865e2c4; BYTE $0xd3 // vfmadd231ps	%ymm3, %ymm3, %ymm2 # ymm2 = (ymm3 * ymm3) + ymm2
LBB1_21:
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xda7cebc5 // vhaddps	%xmm2, %xmm2, %xmm3
	WORD $0x3949; BYTE $0xd0 // cmpq	%rdx, %r8
	JGE LBB1_22
	WORD $0x894d; BYTE $0xc5 // movq	%r8, %r13
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JE LBB1_25
	LONG $0x0d6cbe0f; BYTE $0x08 // movsbl	0x8(%rbp,%rcx), %ebp
	LONG $0xd52ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x6410fac5; WORD $0x208f // vmovss	0x20(%rdi,%rcx,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	WORD $0x8949; BYTE $0xdd // movq	%rbx, %r13
LBB1_25:
	WORD $0x3949; BYTE $0xcb // cmpq	%rcx, %r11
	JE LBB1_28
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB1_27:
	LONG $0x6cbe0f43; WORD $0xff2f // movsbl	-0x1(%r15,%r13), %ebp
	LONG $0xdd2ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0xaf24 // vmovss	(%rdi,%r13,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x107aa1c4; WORD $0xaf64; BYTE $0x04 // vmovss	0x4(%rdi,%r13,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x2cbe0f43; BYTE $0x2f // movsbl	(%r15,%r13), %ebp
	LONG $0xd52ad2c5 // vcvtsi2ss	%ebp, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c58349 // addq	$0x2, %r13
	WORD $0x394c; BYTE $0xea // cmpq	%r13, %rdx
	JNE LBB1_27
	JMP LBB1_28
LBB1_5:
	WORD $0xc031 // xorl	%eax, %eax
LBB1_6:
	LONG $0x117ac1c4; WORD $0x8114 // vmovss	%xmm2, (%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x04 // vmovss	%xmm2, 0x4(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x08 // vmovss	%xmm2, 0x8(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x0c // vmovss	%xmm2, 0xc(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x10 // vmovss	%xmm2, 0x10(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x14 // vmovss	%xmm2, 0x14(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x18 // vmovss	%xmm2, 0x18(%r9,%rax,4)
	LONG $0x117ac1c4; WORD $0x8154; BYTE $0x1c // vmovss	%xmm2, 0x1c(%r9,%rax,4)
	LONG $0x08c08348 // addq	$0x8, %rax
	LONG $0x01003d48; WORD $0x0000 // cmpq	$0x100, %rax            # imm = 0x100
	JNE LBB1_6
LBB1_16:
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·findNearestCentroidInt8Avx2(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ codebook+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ outIndex+40(FP), R9

	BYTE $0x55 // pushq	%rbp
	WORD $0x5741 // pushq	%r15
	WORD $0x5641 // pushq	%r14
	WORD $0x5541 // pushq	%r13
	WORD $0x5441 // pushq	%r12
	BYTE $0x53 // pushq	%rbx
	LONG $0x10ec8348 // subq	$0x10, %rsp
	LONG $0x240c894c // movq	%r9, (%rsp)
	LONG $0x187de2c4; BYTE $0x01 // vbroadcastss	(%rcx), %ymm0
	LONG $0x187dc2c4; BYTE $0x08 // vbroadcastss	(%r8), %ymm1
	LONG $0x08fa8348 // cmpq	$0x8, %rdx
	JGE LBB2_1
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	JLE LBB2_5
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	WORD $0xbe0f; BYTE $0x06 // movsbl	(%rsi), %eax
	LONG $0xd82ae2c5 // vcvtsi2ss	%eax, %xmm3, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x2710fac5 // vmovss	(%rdi), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x01fa8348 // cmpq	$0x1, %rdx
	JE LBB2_13
	LONG $0x0146be0f // movsbl	0x1(%rsi), %eax
	LONG $0xe02ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x04 // vmovss	0x4(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	LONG $0x02fa8348 // cmpq	$0x2, %rdx
	JE LBB2_13
	LONG $0x0246be0f // movsbl	0x2(%rsi), %eax
	LONG $0xe02acac5 // vcvtsi2ss	%eax, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x08 // vmovss	0x8(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	LONG $0x03fa8348 // cmpq	$0x3, %rdx
	JE LBB2_13
	LONG $0x0346be0f // movsbl	0x3(%rsi), %eax
	LONG $0xe02acac5 // vcvtsi2ss	%eax, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x0c // vmovss	0xc(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	LONG $0x04fa8348 // cmpq	$0x4, %rdx
	JE LBB2_13
	LONG $0x0446be0f // movsbl	0x4(%rsi), %eax
	LONG $0xe02acac5 // vcvtsi2ss	%eax, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x10 // vmovss	0x10(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	LONG $0x05fa8348 // cmpq	$0x5, %rdx
	JE LBB2_13
	LONG $0x0546be0f // movsbl	0x5(%rsi), %eax
	LONG $0xe02acac5 // vcvtsi2ss	%eax, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x14 // vmovss	0x14(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
	LONG $0x06fa8348 // cmpq	$0x6, %rdx
	JE LBB2_13
	LONG $0x0646be0f // movsbl	0x6(%rsi), %eax
	LONG $0xe02acac5 // vcvtsi2ss	%eax, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x6f10fac5; BYTE $0x18 // vmovss	0x18(%rdi), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0xb959e2c4; BYTE $0xdc // vfmadd231ss	%xmm4, %xmm4, %xmm3 # xmm3 = (xmm4 * xmm4) + xmm3
LBB2_13:
	LONG $0x2710fac5 // vmovss	(%rdi), %xmm4
	LONG $0x320c8d48 // leaq	(%rdx,%rsi), %rcx
	LONG $0x06c18348 // addq	$0x6, %rcx
	LONG $0x000001be; BYTE $0x00 // movl	$0x1, %esi
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB2_14
LBB2_21:
	LONG $0xdd2ef8c5 // vucomiss	%xmm5, %xmm3
	LONG $0xc6470f48 // cmovaq	%rsi, %rax
	LONG $0xdb5dd2c5 // vminss	%xmm3, %xmm5, %xmm3
	WORD $0xff48; BYTE $0xc6 // incq	%rsi
	WORD $0x0148; BYTE $0xd1 // addq	%rdx, %rcx
	LONG $0x00fe8148; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %rsi            # imm = 0x100
	JE LBB2_22
LBB2_14:
	LONG $0x41be0f44; BYTE $0xfa // movsbl	-0x6(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xe8 // vcvtsi2ss	%r8d, %xmm8, %xmm5
	LONG $0xa979e2c4; BYTE $0xe9 // vfmadd213ss	%xmm1, %xmm0, %xmm5 # xmm5 = (xmm0 * xmm5) + xmm1
	LONG $0xed5cdac5 // vsubss	%xmm5, %xmm4, %xmm5
	LONG $0xa951e2c4; BYTE $0xea // vfmadd213ss	%xmm2, %xmm5, %xmm5 # xmm5 = (xmm5 * xmm5) + xmm2
	LONG $0x01fa8348 // cmpq	$0x1, %rdx
	JE LBB2_21
	LONG $0x41be0f44; BYTE $0xfb // movsbl	-0x5(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x04 // vmovss	0x4(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	LONG $0x02fa8348 // cmpq	$0x2, %rdx
	JE LBB2_21
	LONG $0x41be0f44; BYTE $0xfc // movsbl	-0x4(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x08 // vmovss	0x8(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	LONG $0x03fa8348 // cmpq	$0x3, %rdx
	JE LBB2_21
	LONG $0x41be0f44; BYTE $0xfd // movsbl	-0x3(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x0c // vmovss	0xc(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	LONG $0x04fa8348 // cmpq	$0x4, %rdx
	JE LBB2_21
	LONG $0x41be0f44; BYTE $0xfe // movsbl	-0x2(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x10 // vmovss	0x10(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	LONG $0x05fa8348 // cmpq	$0x5, %rdx
	JE LBB2_21
	LONG $0x41be0f44; BYTE $0xff // movsbl	-0x1(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x14 // vmovss	0x14(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	LONG $0x06fa8348 // cmpq	$0x6, %rdx
	JE LBB2_21
	LONG $0x01be0f44 // movsbl	(%rcx), %r8d
	LONG $0x2a3ac1c4; BYTE $0xf0 // vcvtsi2ss	%r8d, %xmm8, %xmm6
	LONG $0xa979e2c4; BYTE $0xf1 // vfmadd213ss	%xmm1, %xmm0, %xmm6 # xmm6 = (xmm0 * xmm6) + xmm1
	LONG $0x7f10fac5; BYTE $0x18 // vmovss	0x18(%rdi), %xmm7
	LONG $0xf65cc2c5 // vsubss	%xmm6, %xmm7, %xmm6
	LONG $0xb949e2c4; BYTE $0xee // vfmadd231ss	%xmm6, %xmm6, %xmm5 # xmm5 = (xmm6 * xmm6) + xmm5
	JMP LBB2_21
LBB2_1:
	LONG $0xf84a8d48 // leaq	-0x8(%rdx), %rcx
	WORD $0x8949; BYTE $0xc8 // movq	%rcx, %r8
	LONG $0xf8e08349 // andq	$-0x8, %r8
	LONG $0x08508d4d // leaq	0x8(%r8), %r10
	WORD $0x8949; BYTE $0xcb // movq	%rcx, %r11
	LONG $0x03ebc149 // shrq	$0x3, %r11
	WORD $0xff49; BYTE $0xc3 // incq	%r11
	LONG $0xf75a8d48 // leaq	-0x9(%rdx), %rbx
	LONG $0xfee38349 // andq	$-0x2, %r11
	LONG $0x09408d49 // leaq	0x9(%r8), %rax
	LONG $0x24448948; BYTE $0x08 // movq	%rax, 0x8(%rsp)
	LONG $0x087e8d4c // leaq	0x8(%rsi), %r15
	LONG $0x01668d4c // leaq	0x1(%rsi), %r12
	WORD $0xb540; BYTE $0x01 // movb	$0x1, %bpl
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB2_2
LBB2_36:
	WORD $0xff49; BYTE $0xc1 // incq	%r9
	WORD $0x0149; BYTE $0xd7 // addq	%rdx, %r15
	WORD $0x0149; BYTE $0xd4 // addq	%rdx, %r12
	WORD $0xed31 // xorl	%ebp, %ebp
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
	LONG $0x00f98149; WORD $0x0001; BYTE $0x00 // cmpq	$0x100, %r9             # imm = 0x100
	JE LBB2_22
LBB2_2:
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	LONG $0x08f98348 // cmpq	$0x8, %rcx
	JCC LBB2_23
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
	JMP LBB2_25
LBB2_23:
	WORD $0x894d; BYTE $0xdd // movq	%r11, %r13
	WORD $0x3145; BYTE $0xf6 // xorl	%r14d, %r14d
LBB2_24:
	LONG $0x217d82c4; WORD $0x3764; BYTE $0xf8 // vpmovsxbd	-0x8(%r15,%r14), %ymm4
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0xa87de2c4; BYTE $0xe1 // vfmadd213ps	%ymm1, %ymm0, %ymm4 # ymm4 = (ymm0 * ymm4) + ymm1
	LONG $0x107ca1c4; WORD $0xb72c // vmovups	(%rdi,%r14,4), %ymm5
	LONG $0xe45cd4c5 // vsubps	%ymm4, %ymm5, %ymm4
	LONG $0x107ca1c4; WORD $0xb76c; BYTE $0x20 // vmovups	0x20(%rdi,%r14,4), %ymm5
	LONG $0xa85de2c4; BYTE $0xe3 // vfmadd213ps	%ymm3, %ymm4, %ymm4 # ymm4 = (ymm4 * ymm4) + ymm3
	LONG $0x217d82c4; WORD $0x371c // vpmovsxbd	(%r15,%r14), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0xdb5cd4c5 // vsubps	%ymm3, %ymm5, %ymm3
	LONG $0xa865e2c4; BYTE $0xdc // vfmadd213ps	%ymm4, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm4
	LONG $0x10c68349 // addq	$0x10, %r14
	LONG $0xfec58349 // addq	$-0x2, %r13
	JNE LBB2_24
LBB2_25:
	WORD $0x894d; BYTE $0xcd // movq	%r9, %r13
	LONG $0xeaaf0f4c // imulq	%rdx, %r13
	WORD $0x0149; BYTE $0xf5 // addq	%rsi, %r13
	WORD $0xc1f6; BYTE $0x08 // testb	$0x8, %cl
	JNE LBB2_27
	LONG $0x217d82c4; WORD $0x3564; BYTE $0x00 // vpmovsxbd	(%r13,%r14), %ymm4
	LONG $0xe45bfcc5 // vcvtdq2ps	%ymm4, %ymm4
	LONG $0xa87de2c4; BYTE $0xe1 // vfmadd213ps	%ymm1, %ymm0, %ymm4 # ymm4 = (ymm0 * ymm4) + ymm1
	LONG $0x107ca1c4; WORD $0xb72c // vmovups	(%rdi,%r14,4), %ymm5
	LONG $0xe45cd4c5 // vsubps	%ymm4, %ymm5, %ymm4
	LONG $0xb85de2c4; BYTE $0xdc // vfmadd231ps	%ymm4, %ymm4, %ymm3 # ymm3 = (ymm4 * ymm4) + ymm3
LBB2_27:
	LONG $0x197de3c4; WORD $0x01dc // vextractf128	$0x1, %ymm3, %xmm4
	LONG $0xdc58e0c5 // vaddps	%xmm4, %xmm3, %xmm3
	LONG $0xdb7ce3c5 // vhaddps	%xmm3, %xmm3, %xmm3
	LONG $0xe37ce3c5 // vhaddps	%xmm3, %xmm3, %xmm4
	WORD $0x3949; BYTE $0xd2 // cmpq	%rdx, %r10
	JGE LBB2_28
	WORD $0x894d; BYTE $0xd6 // movq	%r10, %r14
	WORD $0xc2f6; BYTE $0x01 // testb	$0x1, %dl
	JE LBB2_31
	LONG $0x74be0f47; WORD $0x0805 // movsbl	0x8(%r13,%r8), %r14d
	LONG $0x2a4ac1c4; BYTE $0xde // vcvtsi2ss	%r14d, %xmm6, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x107aa1c4; WORD $0x876c; BYTE $0x20 // vmovss	0x20(%rdi,%r8,4), %xmm5
	LONG $0xdb5cd2c5 // vsubss	%xmm3, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0xe328f8c5 // vmovaps	%xmm3, %xmm4
	LONG $0x24748b4c; BYTE $0x08 // movq	0x8(%rsp), %r14
LBB2_31:
	WORD $0x394c; BYTE $0xc3 // cmpq	%r8, %rbx
	JE LBB2_34
	LONG $0xdc28f8c5 // vmovaps	%xmm4, %xmm3
LBB2_33:
	LONG $0x6cbe0f47; WORD $0xff34 // movsbl	-0x1(%r12,%r14), %r13d
	LONG $0x2a4ac1c4; BYTE $0xe5 // vcvtsi2ss	%r13d, %xmm6, %xmm4
	LONG $0xa979e2c4; BYTE $0xe1 // vfmadd213ss	%xmm1, %xmm0, %xmm4 # xmm4 = (xmm0 * xmm4) + xmm1
	LONG $0x107aa1c4; WORD $0xb72c // vmovss	(%rdi,%r14,4), %xmm5
	LONG $0xe45cd2c5 // vsubss	%xmm4, %xmm5, %xmm4
	LONG $0x107aa1c4; WORD $0xb76c; BYTE $0x04 // vmovss	0x4(%rdi,%r14,4), %xmm5
	LONG $0xa959e2c4; BYTE $0xe3 // vfmadd213ss	%xmm3, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm3
	LONG $0x2cbe0f47; BYTE $0x34 // movsbl	(%r12,%r14), %r13d
	LONG $0x2a4ac1c4; BYTE $0xdd // vcvtsi2ss	%r13d, %xmm6, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0xdb5cd2c5 // vsubss	%xmm3, %xmm5, %xmm3
	LONG $0xa961e2c4; BYTE $0xdc // vfmadd213ss	%xmm4, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm4
	LONG $0x02c68349 // addq	$0x2, %r14
	WORD $0x394c; BYTE $0xf2 // cmpq	%r14, %rdx
	JNE LBB2_33
	JMP LBB2_34
LBB2_28:
	LONG $0xdc28f8c5 // vmovaps	%xmm4, %xmm3
LBB2_34:
	LONG $0xd32ef8c5 // vucomiss	%xmm3, %xmm2
	LONG $0xc6970f41 // seta	%r14b
	WORD $0x0844; BYTE $0xf5 // orb	%r14b, %bpl
	LONG $0x01c5f640 // testb	$0x1, %bpl
	LONG $0xc1450f49 // cmovneq	%r9, %rax
	JNE LBB2_36
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
	JMP LBB2_36
LBB2_5:
	WORD $0xc031 // xorl	%eax, %eax
LBB2_22:
	LONG $0x240c8b48 // movq	(%rsp), %rcx
	WORD $0x8948; BYTE $0x01 // movq	%rax, (%rcx)
	LONG $0x10c48348 // addq	$0x10, %rsp
	BYTE $0x5b // popq	%rbx
	WORD $0x5c41 // popq	%r12
	WORD $0x5d41 // popq	%r13
	WORD $0x5e41 // popq	%r14
	WORD $0x5f41 // popq	%r15
	BYTE $0x5d // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·squaredL2Int8DequantizedAvx2(SB), NOSPLIT, $0-48
	MOVQ query+0(FP), DI
	MOVQ code+8(FP), SI
	MOVQ subdim+16(FP), DX
	MOVQ scale+24(FP), CX
	MOVQ offset+32(FP), R8
	MOVQ out+40(FP), R9

	LONG $0x187de2c4; BYTE $0x01 // vbroadcastss	(%rcx), %ymm0
	LONG $0x187dc2c4; BYTE $0x08 // vbroadcastss	(%r8), %ymm1
	LONG $0x08fa8348 // cmpq	$0x8, %rdx
	JGE LBB0_2
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0xc031 // xorl	%eax, %eax
	JMP LBB0_9
LBB0_2:
	LONG $0xf84a8d48 // leaq	-0x8(%rdx), %rcx
	LONG $0x08f98348 // cmpq	$0x8, %rcx
	JCC LBB0_4
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc0 // xorl	%r8d, %r8d
	JMP LBB0_6
LBB0_4:
	WORD $0x8948; BYTE $0xc8 // movq	%rcx, %rax
	LONG $0x03e8c148 // shrq	$0x3, %rax
	WORD $0xff48; BYTE $0xc0 // incq	%rax
	LONG $0xfee08348 // andq	$-0x2, %rax
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	WORD $0x3145; BYTE $0xc0 // xorl	%r8d, %r8d
LBB0_5:
	LONG $0x217da2c4; WORD $0x061c // vpmovsxbd	(%rsi,%r8), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0x8724 // vmovups	(%rdi,%r8,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0x107ca1c4; WORD $0x8764; BYTE $0x20 // vmovups	0x20(%rdi,%r8,4), %ymm4
	LONG $0xa865e2c4; BYTE $0xda // vfmadd213ps	%ymm2, %ymm3, %ymm3 # ymm3 = (ymm3 * ymm3) + ymm2
	LONG $0x217da2c4; WORD $0x0654; BYTE $0x08 // vpmovsxbd	0x8(%rsi,%r8), %ymm2
	LONG $0xd25bfcc5 // vcvtdq2ps	%ymm2, %ymm2
	LONG $0xa87de2c4; BYTE $0xd1 // vfmadd213ps	%ymm1, %ymm0, %ymm2 # ymm2 = (ymm0 * ymm2) + ymm1
	LONG $0xd25cdcc5 // vsubps	%ymm2, %ymm4, %ymm2
	LONG $0xa86de2c4; BYTE $0xd3 // vfmadd213ps	%ymm3, %ymm2, %ymm2 # ymm2 = (ymm2 * ymm2) + ymm3
	LONG $0x10c08349 // addq	$0x10, %r8
	LONG $0xfec08348 // addq	$-0x2, %rax
	JNE LBB0_5
LBB0_6:
	WORD $0x8948; BYTE $0xc8 // movq	%rcx, %rax
	LONG $0xf8e08348 // andq	$-0x8, %rax
	WORD $0xc1f6; BYTE $0x08 // testb	$0x8, %cl
	JNE LBB0_8
	LONG $0x217da2c4; WORD $0x061c // vpmovsxbd	(%rsi,%r8), %ymm3
	LONG $0xdb5bfcc5 // vcvtdq2ps	%ymm3, %ymm3
	LONG $0xa87de2c4; BYTE $0xd9 // vfmadd213ps	%ymm1, %ymm0, %ymm3 # ymm3 = (ymm0 * ymm3) + ymm1
	LONG $0x107ca1c4; WORD $0x8724 // vmovups	(%rdi,%r8,4), %ymm4
	LONG $0xdb5cdcc5 // vsubps	%ymm3, %ymm4, %ymm3
	LONG $0xb865e2c4; BYTE $0xd3 // vfmadd231ps	%ymm3, %ymm3, %ymm2 # ymm2 = (ymm3 * ymm3) + ymm2
LBB0_8:
	LONG $0x08c08348 // addq	$0x8, %rax
LBB0_9:
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xda7cebc5 // vhaddps	%xmm2, %xmm2, %xmm3
	WORD $0x3948; BYTE $0xc2 // cmpq	%rax, %rdx
	JLE LBB0_10
	WORD $0x8941; BYTE $0xd0 // movl	%edx, %r8d
	WORD $0x2941; BYTE $0xc0 // subl	%eax, %r8d
	WORD $0x8948; BYTE $0xc1 // movq	%rax, %rcx
	LONG $0x01c0f641 // testb	$0x1, %r8b
	JE LBB0_13
	LONG $0x060cbe0f // movsbl	(%rsi,%rax), %ecx
	LONG $0xd12ad2c5 // vcvtsi2ss	%ecx, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0x2410fac5; BYTE $0x87 // vmovss	(%rdi,%rax,4), %xmm4
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x01488d48 // leaq	0x1(%rax), %rcx
	LONG $0xda28f8c5 // vmovaps	%xmm2, %xmm3
LBB0_13:
	LONG $0xff428d4c // leaq	-0x1(%rdx), %r8
	WORD $0x394c; BYTE $0xc0 // cmpq	%r8, %rax
	JE LBB0_16
	LONG $0xd328f8c5 // vmovaps	%xmm3, %xmm2
LBB0_15:
	LONG $0x0e04be0f // movsbl	(%rsi,%rcx), %eax
	LONG $0xd82ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm3
	LONG $0xa979e2c4; BYTE $0xd9 // vfmadd213ss	%xmm1, %xmm0, %xmm3 # xmm3 = (xmm0 * xmm3) + xmm1
	LONG $0x2410fac5; BYTE $0x8f // vmovss	(%rdi,%rcx,4), %xmm4
	LONG $0xdb5cdac5 // vsubss	%xmm3, %xmm4, %xmm3
	LONG $0x6410fac5; WORD $0x048f // vmovss	0x4(%rdi,%rcx,4), %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x0e44be0f; BYTE $0x01 // movsbl	0x1(%rsi,%rcx), %eax
	LONG $0xd02ad2c5 // vcvtsi2ss	%eax, %xmm5, %xmm2
	LONG $0xa979e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm0, %xmm2 # xmm2 = (xmm0 * xmm2) + xmm1
	LONG $0xd25cdac5 // vsubss	%xmm2, %xmm4, %xmm2
	LONG $0xa969e2c4; BYTE $0xd3 // vfmadd213ss	%xmm3, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm3
	LONG $0x02c18348 // addq	$0x2, %rcx
	WORD $0x3948; BYTE $0xca // cmpq	%rcx, %rdx
	JNE LBB0_15
LBB0_16:
	LONG $0x117ac1c4; BYTE $0x11 // vmovss	%xmm2, (%r9)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET
LBB0_10:
	LONG $0x117ac1c4; BYTE $0x19 // vmovss	%xmm3, (%r9)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

