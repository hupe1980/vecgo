// Code generated by internal/simd/cmd/generator. DO NOT EDIT.

//go:build !noasm && amd64

#include "textflag.h"

TEXT Â·squaredL2BoundedAvx512(SB), NOSPLIT, $0-48
	MOVQ vec1+0(FP), DI
	MOVQ vec2+8(FP), SI
	MOVQ n+16(FP), DX
	MOVSD bound+24(FP), X0
	MOVQ result+32(FP), CX
	MOVQ exceeded+40(FP), R8

	LONG $0xc957f0c5 // vxorps	%xmm1, %xmm1, %xmm1
	LONG $0x000040b8; BYTE $0x00 // movl	$0x40, %eax
	LONG $0xd257e8c5 // vxorps	%xmm2, %xmm2, %xmm2
	LONG $0xdb57e0c5 // vxorps	%xmm3, %xmm3, %xmm3
	LONG $0xe457d8c5 // vxorps	%xmm4, %xmm4, %xmm4
LBB0_1:
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	JGT LBB0_4
	LONG $0x870c180f // prefetcht0	(%rdi,%rax,4)
	LONG $0x860c180f // prefetcht0	(%rsi,%rax,4)
	QUAD $0xfc876c10487cf162 // vmovups	-0x100(%rdi,%rax,4), %zmm5
	QUAD $0xfd877410487cf162 // vmovups	-0xc0(%rdi,%rax,4), %zmm6
	QUAD $0xfe877c10487cf162 // vmovups	-0x80(%rdi,%rax,4), %zmm7
	QUAD $0xff874410487c7162 // vmovups	-0x40(%rdi,%rax,4), %zmm8
	QUAD $0xfc866c5c4854f162 // vsubps	-0x100(%rsi,%rax,4), %zmm5, %zmm5
	QUAD $0xfd86745c484cf162 // vsubps	-0xc0(%rsi,%rax,4), %zmm6, %zmm6
	QUAD $0xfe867c5c4844f162 // vsubps	-0x80(%rsi,%rax,4), %zmm7, %zmm7
	QUAD $0xff86445c483c7162 // vsubps	-0x40(%rsi,%rax,4), %zmm8, %zmm8
	LONG $0x4855f262; WORD $0xe5b8 // vfmadd231ps	%zmm5, %zmm5, %zmm4 # zmm4 = (zmm5 * zmm5) + zmm4
	LONG $0x484df262; WORD $0xdeb8 // vfmadd231ps	%zmm6, %zmm6, %zmm3 # zmm3 = (zmm6 * zmm6) + zmm3
	LONG $0x4845f262; WORD $0xd7b8 // vfmadd231ps	%zmm7, %zmm7, %zmm2 # zmm2 = (zmm7 * zmm7) + zmm2
	LONG $0x483dd262; WORD $0xc8b8 // vfmadd231ps	%zmm8, %zmm8, %zmm1 # zmm1 = (zmm8 * zmm8) + zmm1
	LONG $0x485cf162; WORD $0xeb58 // vaddps	%zmm3, %zmm4, %zmm5
	LONG $0x486cf162; WORD $0xf158 // vaddps	%zmm1, %zmm2, %zmm6
	LONG $0x4854f162; WORD $0xee58 // vaddps	%zmm6, %zmm5, %zmm5
	LONG $0x48fdf362; WORD $0xee1b; BYTE $0x01 // vextractf64x4	$0x1, %zmm5, %ymm6
	LONG $0xee58d4c5 // vaddps	%ymm6, %ymm5, %ymm5
	LONG $0x197de3c4; WORD $0x01ee // vextractf128	$0x1, %ymm5, %xmm6
	LONG $0xee58d0c5 // vaddps	%xmm6, %xmm5, %xmm5
	LONG $0xed7cd3c5 // vhaddps	%xmm5, %xmm5, %xmm5
	LONG $0xed7cd3c5 // vhaddps	%xmm5, %xmm5, %xmm5
	LONG $0x40c08348 // addq	$0x40, %rax
	LONG $0xe82ef8c5 // vucomiss	%xmm0, %xmm5
	JBE LBB0_1
	LONG $0x2911fac5 // vmovss	%xmm5, (%rcx)
	LONG $0x000001b8; BYTE $0x00 // movl	$0x1, %eax
	WORD $0x8941; BYTE $0x00 // movl	%eax, (%r8)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET
LBB0_4:
	LONG $0x4864f162; WORD $0xdc58 // vaddps	%zmm4, %zmm3, %zmm3
	LONG $0x4874f162; WORD $0xca58 // vaddps	%zmm2, %zmm1, %zmm1
	LONG $0x4874f162; WORD $0xcb58 // vaddps	%zmm3, %zmm1, %zmm1
	LONG $0x48fdf362; WORD $0xca1b; BYTE $0x01 // vextractf64x4	$0x1, %zmm1, %ymm2
	LONG $0xca58f4c5 // vaddps	%ymm2, %ymm1, %ymm1
	LONG $0x197de3c4; WORD $0x01ca // vextractf128	$0x1, %ymm1, %xmm2
	LONG $0xca58f0c5 // vaddps	%xmm2, %xmm1, %xmm1
	LONG $0xc97cf3c5 // vhaddps	%xmm1, %xmm1, %xmm1
	LONG $0xc97cf3c5 // vhaddps	%xmm1, %xmm1, %xmm1
	LONG $0xc8488d4c // leaq	-0x38(%rax), %r9
	LONG $0xc0c08348 // addq	$-0x40, %rax
	WORD $0x3949; BYTE $0xd1 // cmpq	%rdx, %r9
	JLE LBB0_13
	WORD $0x8949; BYTE $0xc1 // movq	%rax, %r9
	JMP LBB0_6
LBB0_13:
	LONG $0x1410fcc5; BYTE $0x87 // vmovups	(%rdi,%rax,4), %ymm2
	LONG $0x145cecc5; BYTE $0x86 // vsubps	(%rsi,%rax,4), %ymm2, %ymm2
	LONG $0xd259ecc5 // vmulps	%ymm2, %ymm2, %ymm2
	LONG $0x197de3c4; WORD $0x01d3 // vextractf128	$0x1, %ymm2, %xmm3
	LONG $0xd358e8c5 // vaddps	%xmm3, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xd27cebc5 // vhaddps	%xmm2, %xmm2, %xmm2
	LONG $0xca58f2c5 // vaddss	%xmm2, %xmm1, %xmm1
	LONG $0x08488d4c // leaq	0x8(%rax), %r9
	LONG $0x10c08348 // addq	$0x10, %rax
	WORD $0x3948; BYTE $0xd0 // cmpq	%rdx, %rax
	WORD $0x894c; BYTE $0xc8 // movq	%r9, %rax
	JLE LBB0_13
LBB0_6:
	WORD $0x894c; BYTE $0xc8 // movq	%r9, %rax
	WORD $0x2948; BYTE $0xd0 // subq	%rdx, %rax
	JGE LBB0_11
	WORD $0x8941; BYTE $0xd2 // movl	%edx, %r10d
	WORD $0x2945; BYTE $0xca // subl	%r9d, %r10d
	LONG $0x03e28341 // andl	$0x3, %r10d
	JE LBB0_9
LBB0_8:
	LONG $0x107aa1c4; WORD $0x8f14 // vmovss	(%rdi,%r9,4), %xmm2
	LONG $0x5c6aa1c4; WORD $0x8e14 // vsubss	(%rsi,%r9,4), %xmm2, %xmm2
	LONG $0xb969e2c4; BYTE $0xca // vfmadd231ss	%xmm2, %xmm2, %xmm1 # xmm1 = (xmm2 * xmm2) + xmm1
	WORD $0xff49; BYTE $0xc1 // incq	%r9
	WORD $0xff49; BYTE $0xca // decq	%r10
	JNE LBB0_8
LBB0_9:
	LONG $0xfcf88348 // cmpq	$-0x4, %rax
	JHI LBB0_11
LBB0_10:
	LONG $0x107aa1c4; WORD $0x8f14 // vmovss	(%rdi,%r9,4), %xmm2
	LONG $0x107aa1c4; WORD $0x8f5c; BYTE $0x04 // vmovss	0x4(%rdi,%r9,4), %xmm3
	LONG $0x5c6aa1c4; WORD $0x8e14 // vsubss	(%rsi,%r9,4), %xmm2, %xmm2
	LONG $0x5c62a1c4; WORD $0x8e5c; BYTE $0x04 // vsubss	0x4(%rsi,%r9,4), %xmm3, %xmm3
	LONG $0xa969e2c4; BYTE $0xd1 // vfmadd213ss	%xmm1, %xmm2, %xmm2 # xmm2 = (xmm2 * xmm2) + xmm1
	LONG $0x107aa1c4; WORD $0x8f4c; BYTE $0x08 // vmovss	0x8(%rdi,%r9,4), %xmm1
	LONG $0x5c72a1c4; WORD $0x8e64; BYTE $0x08 // vsubss	0x8(%rsi,%r9,4), %xmm1, %xmm4
	LONG $0xa961e2c4; BYTE $0xda // vfmadd213ss	%xmm2, %xmm3, %xmm3 # xmm3 = (xmm3 * xmm3) + xmm2
	LONG $0x107aa1c4; WORD $0x8f4c; BYTE $0x0c // vmovss	0xc(%rdi,%r9,4), %xmm1
	LONG $0x5c72a1c4; WORD $0x8e4c; BYTE $0x0c // vsubss	0xc(%rsi,%r9,4), %xmm1, %xmm1
	LONG $0xa959e2c4; BYTE $0xe3 // vfmadd213ss	%xmm3, %xmm4, %xmm4 # xmm4 = (xmm4 * xmm4) + xmm3
	LONG $0xa971e2c4; BYTE $0xcc // vfmadd213ss	%xmm4, %xmm1, %xmm1 # xmm1 = (xmm1 * xmm1) + xmm4
	LONG $0x04c18349 // addq	$0x4, %r9
	WORD $0x394c; BYTE $0xca // cmpq	%r9, %rdx
	JNE LBB0_10
LBB0_11:
	LONG $0x0911fac5 // vmovss	%xmm1, (%rcx)
	WORD $0xc031 // xorl	%eax, %eax
	LONG $0xc82ef8c5 // vucomiss	%xmm0, %xmm1
	WORD $0x970f; BYTE $0xc0 // seta	%al
	WORD $0x8941; BYTE $0x00 // movl	%eax, (%r8)
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

